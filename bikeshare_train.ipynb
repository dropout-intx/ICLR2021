{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from xgboost import XGBRegressor as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
       "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
       "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
       "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
       "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
       "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
       "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
       "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
       "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data/Bikeshare/hour.csv\", header=0, sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.where(df['dteday'].values < '2012-07-01')[0]\n",
    "test_idx = np.where(df['dteday'].values >= '2012-07-01')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant  dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1       0       1   0     1   0        0        6           0   \n",
       "1        2       0       1   0     1   1        0        6           0   \n",
       "2        3       0       1   0     1   2        0        6           0   \n",
       "3        4       0       1   0     1   3        0        6           0   \n",
       "4        5       0       1   0     1   4        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
       "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
       "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
       "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
       "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dteday'] = [sorted(set(df['dteday'].values)).index(x) for x in df['dteday'].values]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dteday  season  yr  mnth  hr  holiday  weekday  workingday  weathersit  \\\n",
      "0       0       1   0     1   0        0        6           0           1   \n",
      "1       0       1   0     1   1        0        6           0           1   \n",
      "2       0       1   0     1   2        0        6           0           1   \n",
      "3       0       1   0     1   3        0        6           0           1   \n",
      "4       0       1   0     1   4        0        6           0           1   \n",
      "\n",
      "   temp   atemp   hum  windspeed  \n",
      "0  0.24  0.2879  0.81        0.0  \n",
      "1  0.22  0.2727  0.80        0.0  \n",
      "2  0.22  0.2727  0.80        0.0  \n",
      "3  0.24  0.2879  0.75        0.0  \n",
      "4  0.24  0.2879  0.75        0.0  \n"
     ]
    }
   ],
   "source": [
    "Y = df['cnt'].values\n",
    "X = df.drop(['cnt', 'instant', 'casual', 'registered'], axis=1)\n",
    "print(X.head())\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_time = True\n",
    "if split_time:\n",
    "    X_train = X[train_idx]\n",
    "    X_test  = X[test_idx]\n",
    "    Y_train = Y[train_idx]\n",
    "    Y_test  = Y[test_idx]\n",
    "    X_query = X.copy()\n",
    "else:\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n",
    "    X_query = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_mean = np.mean(Y_train)\n",
    "Y_std  = np.std(Y_train)\n",
    "Y_train = (Y_train - Y_mean) / Y_std\n",
    "Y_test  = (Y_test  - Y_mean) / Y_std\n",
    "\n",
    "X_means = np.mean(X_train, axis=0)\n",
    "X_stds = np.std(X_train, axis=0)\n",
    "X_train = (X_train - X_means) / X_stds\n",
    "X_test = (X_test - X_means) / X_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13003,) (4376,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.94361305 -0.79434011 -0.84409775 ...  0.28166902  0.02666108\n",
      " -0.09773304]\n",
      "[-0.11639216 -0.46469569 -0.48335481 ... -0.48335481 -0.66372628\n",
      " -0.73836275]\n",
      "1.0\n",
      "1.3664198073290525\n"
     ]
    }
   ],
   "source": [
    "print(Y_train)\n",
    "print(Y_test)\n",
    "print(np.std(Y_train))\n",
    "print(np.std(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.291825726095841\n",
      "0.8398885664517867\n"
     ]
    }
   ],
   "source": [
    "xgb1 = xgb(max_depth=1, n_estimators=2000)\n",
    "xgb1.fit(X_train, Y_train)\n",
    "print(np.mean(np.square(Y_train - xgb1.predict(X_train))))\n",
    "print(np.mean(np.square(Y_test - xgb1.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.138394253908292\n",
      "0.5167277562993031\n"
     ]
    }
   ],
   "source": [
    "xgb2 = xgb(max_depth=2, n_estimators=1000)\n",
    "xgb2.fit(X_train, Y_train)\n",
    "print(np.mean(np.square(Y_train - xgb2.predict(X_train))))\n",
    "print(np.mean(np.square(Y_test - xgb2.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04979640057972824\n",
      "0.29513703662409957\n"
     ]
    }
   ],
   "source": [
    "xgb3 = xgb(max_depth=3, n_estimators=1000)\n",
    "xgb3.fit(X_train, Y_train)\n",
    "print(np.mean(np.square(Y_train - xgb3.predict(X_train))))\n",
    "print(np.mean(np.square(Y_test - xgb3.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02554988638705647\n",
      "0.24579002239462971\n"
     ]
    }
   ],
   "source": [
    "xgb4 = xgb(max_depth=4, n_estimators=1000)\n",
    "xgb4.fit(X_train, Y_train)\n",
    "print(np.mean(np.square(Y_train - xgb4.predict(X_train))))\n",
    "print(np.mean(np.square(Y_test - xgb4.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000002\n",
      "1.867103089861165\n"
     ]
    }
   ],
   "source": [
    "print(np.var(Y_train))\n",
    "print(np.var(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(activation, dropout_rate=0.0, n_hidden=128):\n",
    "    # TODO: More layers? Different archs?\n",
    "    print(\"Building model with {} activation and {:.3f} dropout\".format(activation, dropout_rate))\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(13,)),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(n_hidden, activation=activation),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(n_hidden, activation=activation),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(n_hidden, activation=activation),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(n_hidden, activation=activation),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(1, activation=tf.identity)\n",
    "    ])\n",
    "    sgd = tf.keras.optimizers.Adam(lr=1e-3)#, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    if activation == 'sigmoid':\n",
    "        model.compile(\n",
    "                      loss='MSE',\n",
    "                      metrics=['mse'],\n",
    "                     optimizer=sgd)\n",
    "    else:\n",
    "        model.compile(\n",
    "              loss='MSE',\n",
    "              metrics=['mse'],\n",
    "                     optimizer=sgd)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with sigmoid activation and 0.000 dropout\n",
      "0.0 0 0\n",
      "WARNING:tensorflow:Layer flatten_64 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "3.011575e-06 1.1730004e-08 6.967876e-09 1.3417462996397012 3.127217302685323\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 89us/sample - loss: 0.7233 - mse: 0.7233\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 74us/sample - loss: 0.6097 - mse: 0.6097\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.6005 - mse: 0.6005\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.5904 - mse: 0.5904\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.5493 - mse: 0.5493\n",
      "0.0 0 5\n",
      "0.6215294 0.005030761 0.00080280047 0.5079521523320112 1.0866444988829387\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.4729 - mse: 0.4729\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 81us/sample - loss: 0.4277 - mse: 0.4277\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 65us/sample - loss: 0.3902 - mse: 0.3902\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 77us/sample - loss: 0.3405 - mse: 0.3405\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 65us/sample - loss: 0.2825 - mse: 0.2825\n",
      "0.0 0 10\n",
      "0.8116332 0.0693634 0.025728375 0.24176287525943616 0.5920049165092397\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 66us/sample - loss: 0.2132 - mse: 0.2132\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.1681 - mse: 0.1681\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.1523 - mse: 0.1523\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.1427 - mse: 0.1427\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.1355 - mse: 0.1355\n",
      "0.0 0 15\n",
      "1.0194951 0.10097555 0.10158992 0.1489700563356247 0.35138865031100225\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 66us/sample - loss: 0.1272 - mse: 0.1272\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.1199 - mse: 0.1199\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.1143 - mse: 0.1143\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.1098 - mse: 0.1098\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.1031 - mse: 0.1031\n",
      "0.0 0 20\n",
      "0.9097141 0.1139106 0.09276623 0.10503592227890617 0.27599298147773843\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.0949 - mse: 0.0949\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.0895 - mse: 0.0895\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 62us/sample - loss: 0.0866 - mse: 0.0866\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 67us/sample - loss: 0.0859 - mse: 0.0859\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 82us/sample - loss: 0.0802 - mse: 0.08020s - loss: 0\n",
      "0.0 0 25\n",
      "1.0938787 0.14392522 0.09222301 0.08575675891314352 0.22814918334988354\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.0759 - mse: 0.0759\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.0747 - mse: 0.0747\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 68us/sample - loss: 0.0720 - mse: 0.07200s - loss: 0.0721 - mse: 0.\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.0724 - mse: 0.0724\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.0723 - mse: 0.0723\n",
      "0.0 0 30\n",
      "0.8122909 0.12523352 0.066366024 0.07290338650267701 0.30096412484229357\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.0695 - mse: 0.0695\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 64us/sample - loss: 0.0667 - mse: 0.0667\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 67us/sample - loss: 0.0672 - mse: 0.0672\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 66us/sample - loss: 0.0674 - mse: 0.06740s - loss: 0.0661 -\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.0641 - mse: 0.0641\n",
      "0.0 0 35\n",
      "0.8275876 0.16441919 0.040161487 0.06492053005762043 0.3069447626916932\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.0657 - mse: 0.0657\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 44us/sample - loss: 0.0658 - mse: 0.0658\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.0625 - mse: 0.0625\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.0623 - mse: 0.0623\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.0627 - mse: 0.0627\n",
      "0.0 0 40\n",
      "0.79371727 0.11509349 0.08326115 0.11060487038061842 0.3839793458303666\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 62us/sample - loss: 0.0628 - mse: 0.0628\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 43us/sample - loss: 0.0627 - mse: 0.0627\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.0602 - mse: 0.0602\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 43us/sample - loss: 0.0616 - mse: 0.0616\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.0600 - mse: 0.0600\n",
      "0.0 0 45\n",
      "0.8232841 0.118639514 0.071233116 0.06096184970747241 0.27963142271401953\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 72us/sample - loss: 0.0590 - mse: 0.05900s - loss: 0.0588 - mse: 0.058 - ETA: 0s - loss: 0.0585 - mse: 0.05\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 67us/sample - loss: 0.0589 - mse: 0.0589\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 64us/sample - loss: 0.0583 - mse: 0.0583\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 2s 155us/sample - loss: 0.0581 - mse: 0.0581\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 86us/sample - loss: 0.0588 - mse: 0.0588\n",
      "0.0 0 50\n",
      "0.8323143 0.15003249 0.051860448 0.05488538879807141 0.27822780043942724\n",
      "0.0 0 75\n",
      "0.87595135 0.12816347 0.07340834 0.04921591774872526 0.24355339731634373\n",
      "0.0 0 100\n",
      "0.8429559 0.17211968 0.080173984 0.0427316256695188 0.29371754743213946\n",
      "0.0 0 125\n",
      "0.84667474 0.14245547 0.0989272 0.038673335062249216 0.2905336792464328\n",
      "0.0 0 150\n",
      "0.95854896 0.15848808 0.09674439 0.04028959529166922 0.24451441327087783\n",
      "0.0 0 175\n",
      "0.8686094 0.14826363 0.086039804 0.0326488872908028 0.2832174443833395\n",
      "0.0 0 200\n",
      "0.89279383 0.1541961 0.103852175 0.030636097422537194 0.2979925639868817\n",
      "0.0 0 225\n",
      "1.0240173 0.18076122 0.101355314 0.03109706918691543 0.2306916816152448\n",
      "0.0 0 250\n",
      "1.0073651 0.19597618 0.102433965 0.026860594043590022 0.24996651937419886\n",
      "0.0 0 275\n",
      "0.94499034 0.15754628 0.11986879 0.026010201012299197 0.2544115938478485\n",
      "0.0 0 300\n",
      "1.0092669 0.16824938 0.11179792 0.02299459943100977 0.2538249313101372\n",
      "0.0 0 325\n",
      "1.0887318 0.1873083 0.14608392 0.02355584549368384 0.26382648048320484\n",
      "0.0 0 350\n",
      "0.9981915 0.20494793 0.115883395 0.020882548905599086 0.274787196213374\n",
      "0.0 0 375\n",
      "1.0164043 0.20384423 0.11061083 0.018886375894518304 0.29206233280818306\n",
      "0.0 0 400\n",
      "1.0006173 0.1782145 0.11315692 0.018512111874692042 0.30488547415902434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0 425\n",
      "0.9816805 0.19967252 0.11020377 0.016204562627547422 0.30108086872479617\n",
      "0.0 0 450\n",
      "1.0455962 0.20893724 0.09539155 0.0178087516960296 0.2982684877846089\n",
      "0.0 0 475\n",
      "1.0539359 0.22104819 0.095842905 0.014894483634738182 0.293503205450913\n",
      "Building model with sigmoid activation and 0.050 dropout\n",
      "0.05 0 0\n",
      "WARNING:tensorflow:Layer flatten_65 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1.441074e-06 1.003799e-08 4.4356963e-09 1.2319988541270008 1.8704885495935852\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 78us/sample - loss: 0.7429 - mse: 0.7429\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.6394 - mse: 0.63940s - loss: 0.6645 \n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.6308 - mse: 0.6308\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 67us/sample - loss: 0.6205 - mse: 0.6205\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.5918 - mse: 0.5918\n",
      "0.05 0 5\n",
      "0.3876446 0.0038685394 0.00070679863 0.5689536601477835 1.2226200295673955\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.5589 - mse: 0.5589\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.4984 - mse: 0.4984\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.4698 - mse: 0.4698\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.4390 - mse: 0.4390\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.3953 - mse: 0.3953\n",
      "0.05 0 10\n",
      "0.80888075 0.06641969 0.011492078 0.3031489566940179 0.7211445201718277\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.3362 - mse: 0.3362\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.2817 - mse: 0.2817\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.2598 - mse: 0.2598\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.2529 - mse: 0.2529\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.2377 - mse: 0.2377\n",
      "0.05 0 15\n",
      "1.0121459 0.12250888 0.057026464 0.14708763631258734 0.3792149773519967\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.2327 - mse: 0.2327\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.2244 - mse: 0.2244\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.2199 - mse: 0.2199\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.2066 - mse: 0.20660s - loss: 0.2153 - ms\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 70us/sample - loss: 0.2039 - mse: 0.2039\n",
      "0.05 0 20\n",
      "1.1850423 0.1331856 0.06889988 0.1393062066179848 0.33810117626429415\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.1914 - mse: 0.1914\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 85us/sample - loss: 0.1935 - mse: 0.1935\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.1870 - mse: 0.1870\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.1832 - mse: 0.1832\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.1779 - mse: 0.1779\n",
      "0.05 0 25\n",
      "0.8770688 0.13694741 0.05651396 0.1084352340847639 0.34196590721590786\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.1782 - mse: 0.1782\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 61us/sample - loss: 0.1790 - mse: 0.1790\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.1724 - mse: 0.1724\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 80us/sample - loss: 0.1744 - mse: 0.1744\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 77us/sample - loss: 0.1780 - mse: 0.1780\n",
      "0.05 0 30\n",
      "0.84834623 0.123370655 0.05784786 0.11048789285370483 0.3991319832951192\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 66us/sample - loss: 0.1717 - mse: 0.1717\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 90us/sample - loss: 0.1683 - mse: 0.1683\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 85us/sample - loss: 0.1676 - mse: 0.1676\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 95us/sample - loss: 0.1663 - mse: 0.1663\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 85us/sample - loss: 0.1679 - mse: 0.1679\n",
      "0.05 0 35\n",
      "0.9025909 0.12096197 0.06560976 0.10181129876613171 0.3734525100831227\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 2s 121us/sample - loss: 0.1555 - mse: 0.1555s - loss: 0.1547 - mse: 0\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 66us/sample - loss: 0.1557 - mse: 0.1557\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 91us/sample - loss: 0.1599 - mse: 0.1599\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.1578 - mse: 0.1578\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.1506 - mse: 0.1506\n",
      "0.05 0 40\n",
      "0.8297847 0.10705943 0.0552765 0.08299629771246482 0.3434783677117462\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 65us/sample - loss: 0.1543 - mse: 0.1543\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 70us/sample - loss: 0.1516 - mse: 0.1516\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 66us/sample - loss: 0.1477 - mse: 0.1477\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 65us/sample - loss: 0.1505 - mse: 0.1505\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.1508 - mse: 0.1508\n",
      "0.05 0 45\n",
      "0.9031312 0.11022331 0.05892961 0.09191611058634828 0.35572221512372815\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.1545 - mse: 0.1545\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.1520 - mse: 0.1520\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.1504 - mse: 0.1504\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.1494 - mse: 0.1494\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.1481 - mse: 0.1481\n",
      "0.05 0 50\n",
      "0.8641892 0.12879004 0.057663556 0.08833541243382258 0.358548772551604\n",
      "0.05 0 75\n",
      "0.6764129 0.088928536 0.052660286 0.08085630129470149 0.533064350518516\n",
      "0.05 0 100\n",
      "0.7732787 0.1048865 0.06438528 0.08211122757785638 0.4239461280368718\n",
      "0.05 0 125\n",
      "0.63761044 0.09243638 0.06487127 0.07967276842372271 0.5147688666703493\n",
      "0.05 0 150\n",
      "0.70995235 0.12487441 0.0473124 0.05610330325037704 0.4305354676633974\n",
      "0.05 0 175\n",
      "0.77817637 0.10642869 0.08170666 0.0635260571190503 0.4249370956462514\n",
      "0.05 0 200\n",
      "0.81852406 0.11296058 0.08661098 0.06124827216929284 0.40526111991624597\n",
      "0.05 0 225\n",
      "0.8106244 0.12383832 0.08733318 0.052905209497180285 0.35382523862996385\n",
      "0.05 0 250\n",
      "0.75359905 0.117420636 0.065531634 0.05248978896421999 0.40388414821355384\n",
      "0.05 0 275\n",
      "0.8029879 0.12601966 0.07201864 0.048796045087615036 0.35948262452830776\n",
      "0.05 0 300\n",
      "0.7271951 0.13283513 0.05179996 0.04664590425268584 0.39899967402646686\n",
      "0.05 0 325\n",
      "0.8171391 0.120394886 0.086835675 0.04583994218294415 0.3512337577563811\n",
      "0.05 0 350\n",
      "0.7328112 0.11793529 0.051125806 0.040558499229953235 0.39835341178894645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 0 375\n",
      "0.8163718 0.14618123 0.06308964 0.03824791389239821 0.3293885365171417\n",
      "0.05 0 400\n",
      "0.7220243 0.16150023 0.052256644 0.04540639804750721 0.4273269518197794\n",
      "0.05 0 425\n",
      "0.77184695 0.12386516 0.061992053 0.039552473413843964 0.3841679527187933\n",
      "0.05 0 450\n",
      "0.88013774 0.14197761 0.07372415 0.036091498061698854 0.3461116640019691\n",
      "0.05 0 475\n",
      "0.8295229 0.14727496 0.05047323 0.03561896575091609 0.3507056557561654\n",
      "Building model with sigmoid activation and 0.125 dropout\n",
      "0.125 0 0\n",
      "WARNING:tensorflow:Layer flatten_66 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2.3332777e-06 1.0930964e-08 6.640508e-09 1.0249956836857264 2.3468994482260985\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 78us/sample - loss: 0.7926 - mse: 0.7926\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6620 - mse: 0.6620\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.6482 - mse: 0.6482\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.6301 - mse: 0.6301\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.6113 - mse: 0.6113\n",
      "0.125 0 5\n",
      "0.37291923 0.0039412812 0.00061999593 0.566200151218875 1.2149764174323532\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 84us/sample - loss: 0.5820 - mse: 0.5820\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 2s 125us/sample - loss: 0.5413 - mse: 0.5413s - loss: 0.\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 90us/sample - loss: 0.5195 - mse: 0.5195\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 102us/sample - loss: 0.5083 - mse: 0.5083\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 98us/sample - loss: 0.4846 - mse: 0.4846\n",
      "0.125 0 10\n",
      "0.7735569 0.048727117 0.006936286 0.4041976433339131 0.9309185676251341\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 114us/sample - loss: 0.4741 - mse: 0.4741\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 96us/sample - loss: 0.4459 - mse: 0.4459\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 69us/sample - loss: 0.3964 - mse: 0.3964\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 76us/sample - loss: 0.3673 - mse: 0.3673\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.3418 - mse: 0.3418\n",
      "0.125 0 15\n",
      "0.95255506 0.107442476 0.03258143 0.2370428784796817 0.5817406769308849\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.3427 - mse: 0.3427\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 78us/sample - loss: 0.3224 - mse: 0.3224\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.3176 - mse: 0.31760s - loss: 0.3152 - mse: 0.315 - ETA: 0s - loss: 0.3168 - mse\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.3187 - mse: 0.3187\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.3113 - mse: 0.3113\n",
      "0.125 0 20\n",
      "0.76968837 0.093003936 0.032120727 0.19880892974867134 0.5683833428473472\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.3046 - mse: 0.3046\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 89us/sample - loss: 0.3032 - mse: 0.3032\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 76us/sample - loss: 0.2877 - mse: 0.2877\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.2928 - mse: 0.2928\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 69us/sample - loss: 0.2901 - mse: 0.2901\n",
      "0.125 0 25\n",
      "0.8933236 0.10164878 0.042700972 0.1969789681835095 0.5556561194450236\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 73us/sample - loss: 0.2811 - mse: 0.2811\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 64us/sample - loss: 0.2772 - mse: 0.2772\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.2708 - mse: 0.2708\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.2648 - mse: 0.2648\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.2697 - mse: 0.2697\n",
      "0.125 0 30\n",
      "0.8368382 0.094927564 0.03880078 0.17934205263917183 0.5272223367854116\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 63us/sample - loss: 0.2641 - mse: 0.2641\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 62us/sample - loss: 0.2584 - mse: 0.2584\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 71us/sample - loss: 0.2568 - mse: 0.2568\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.2608 - mse: 0.2608\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.2641 - mse: 0.2641\n",
      "0.125 0 35\n",
      "0.7301817 0.081395 0.046688326 0.19196044514424557 0.6161239860872009\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 66us/sample - loss: 0.2532 - mse: 0.2532\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 97us/sample - loss: 0.2521 - mse: 0.2521\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 68us/sample - loss: 0.2465 - mse: 0.2465\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 73us/sample - loss: 0.2382 - mse: 0.2382\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 64us/sample - loss: 0.2426 - mse: 0.2426\n",
      "0.125 0 40\n",
      "0.7852756 0.101532735 0.047420494 0.2056358249234599 0.6123702064151203\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 64us/sample - loss: 0.2474 - mse: 0.2474\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.2421 - mse: 0.2421\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.2449 - mse: 0.2449\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.2392 - mse: 0.23920s - loss: 0.2417 - mse\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.2452 - mse: 0.2452\n",
      "0.125 0 45\n",
      "0.73697406 0.099927634 0.050176207 0.1945416270351331 0.7057916867211741\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.2369 - mse: 0.2369\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.2298 - mse: 0.2298\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.2349 - mse: 0.2349\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 66us/sample - loss: 0.2331 - mse: 0.2331\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.2338 - mse: 0.2338\n",
      "0.125 0 50\n",
      "0.72175366 0.10748984 0.05131721 0.20196877763304477 0.6952189572956994\n",
      "0.125 0 75\n",
      "0.7698712 0.10406718 0.057108656 0.21146027259164435 0.6479019083545047\n",
      "0.125 0 100\n",
      "0.75826293 0.11534238 0.06386735 0.18084317873925754 0.60598871127828\n",
      "0.125 0 125\n",
      "0.7693335 0.12922019 0.0601772 0.20547663526986218 0.6390102099616546\n",
      "0.125 0 150\n",
      "0.74974716 0.12421374 0.06308457 0.18068550158284627 0.6314002577130934\n",
      "0.125 0 175\n",
      "0.7660447 0.14273459 0.083407566 0.2291550719152576 0.7006506573125549\n",
      "0.125 0 200\n",
      "0.76148945 0.1294345 0.07957744 0.17896627685231337 0.5692124097906992\n",
      "0.125 0 225\n",
      "0.7035862 0.122189425 0.07002271 0.18627936001808806 0.698370971955386\n",
      "0.125 0 250\n",
      "0.78086233 0.12925404 0.079892255 0.18534023861264728 0.5839858975948602\n",
      "0.125 0 275\n",
      "0.70419914 0.12468704 0.06699611 0.18786298257053988 0.6371292795554718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125 0 300\n",
      "0.70992893 0.114390485 0.07970229 0.1834216517765748 0.6774035999036083\n",
      "0.125 0 325\n",
      "0.7621325 0.11456015 0.078350134 0.17993744583317553 0.5909834298512652\n",
      "0.125 0 350\n",
      "0.71619433 0.11840513 0.07425235 0.19250374012401816 0.6568881330187369\n",
      "0.125 0 375\n",
      "0.71135294 0.14237176 0.07386554 0.16530277036406404 0.5873056437915629\n",
      "0.125 0 400\n",
      "0.7660814 0.13639398 0.072346345 0.19399562017962219 0.6317327064262556\n",
      "0.125 0 425\n",
      "0.6963667 0.117155865 0.07071668 0.16001606341559366 0.6369805237682861\n",
      "0.125 0 450\n",
      "0.70636314 0.13004535 0.08671193 0.21166386964384099 0.6934059903676455\n",
      "0.125 0 475\n",
      "0.72856206 0.12457312 0.071034074 0.16225657531843526 0.6186013839351883\n",
      "Building model with sigmoid activation and 0.250 dropout\n",
      "0.25 0 0\n",
      "WARNING:tensorflow:Layer flatten_67 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "5.805611e-07 6.391771e-09 3.0471912e-09 3.682062185265898 6.593282829795034\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 78us/sample - loss: 0.9326 - mse: 0.9326\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 69us/sample - loss: 0.7070 - mse: 0.70700s - loss: 0.7136 - mse\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.6925 - mse: 0.6925\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.6800 - mse: 0.6800\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.6576 - mse: 0.6576\n",
      "0.25 0 5\n",
      "0.28413045 0.0036331706 0.00055254507 0.568013854859304 1.2819355741403895\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.6419 - mse: 0.6419\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.6269 - mse: 0.6269\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.6009 - mse: 0.6009\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.5839 - mse: 0.5839\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.5735 - mse: 0.5735\n",
      "0.25 0 10\n",
      "0.5100712 0.024073701 0.0023313423 0.46476210387016853 1.0618803453800518\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.5678 - mse: 0.5678\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.5584 - mse: 0.5584\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.5518 - mse: 0.5518\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.5320 - mse: 0.5320\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.5196 - mse: 0.5196\n",
      "0.25 0 15\n",
      "0.61355436 0.040997624 0.00580531 0.415648086913143 0.9906490189163828\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.5026 - mse: 0.5026\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.4907 - mse: 0.4907\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.4721 - mse: 0.4721\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.4572 - mse: 0.4572\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.4481 - mse: 0.4481\n",
      "0.25 0 20\n",
      "0.712269 0.07582937 0.019976027 0.41359285239200533 0.9659314659867433\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.4444 - mse: 0.4444\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.4261 - mse: 0.4261\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.4281 - mse: 0.4281\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.4230 - mse: 0.4230\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.4309 - mse: 0.4309\n",
      "0.25 0 25\n",
      "0.63875633 0.06534892 0.02497301 0.43947565370335634 1.0691413468050617\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.4184 - mse: 0.4184\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.4154 - mse: 0.4154\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.4161 - mse: 0.4161\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.4133 - mse: 0.4133\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.4057 - mse: 0.4057\n",
      "0.25 0 30\n",
      "0.6356484 0.064143375 0.026961988 0.39909347279709034 1.0195118828187804\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.4064 - mse: 0.4064\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.3968 - mse: 0.3968\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.3973 - mse: 0.3973\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.4032 - mse: 0.4032\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.3942 - mse: 0.39420s - loss: 0.3907 - mse: 0\n",
      "0.25 0 35\n",
      "0.66972035 0.07048298 0.024285018 0.38511846424009544 1.0066030019647854\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.3898 - mse: 0.3898\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.3967 - mse: 0.3967\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.3920 - mse: 0.3920\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.3900 - mse: 0.39000s - loss: 0.3905 - mse: 0.390\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 69us/sample - loss: 0.3915 - mse: 0.3915\n",
      "0.25 0 40\n",
      "0.8120031 0.089124665 0.031534415 0.42889589256965294 1.0349373885269044\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.3867 - mse: 0.3867\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.3837 - mse: 0.3837\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.3761 - mse: 0.3761\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.3777 - mse: 0.3777\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.3715 - mse: 0.3715\n",
      "0.25 0 45\n",
      "0.59699273 0.062115178 0.02761567 0.400589828391146 1.023989968525168\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.3756 - mse: 0.3756\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.3791 - mse: 0.3791\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.3734 - mse: 0.37340s - loss: 0.3690 - mse: 0.3\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.3730 - mse: 0.3730\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.3721 - mse: 0.3721\n",
      "0.25 0 50\n",
      "0.7371827 0.07859583 0.041922245 0.4939756858774502 1.120293238855049\n",
      "0.25 0 75\n",
      "0.7182133 0.07516112 0.041072935 0.4849966737980612 1.1685449053533554\n",
      "0.25 0 100\n",
      "0.7149968 0.102463424 0.052287176 0.5260950316930892 1.140855780740136\n",
      "0.25 0 125\n",
      "0.7361178 0.10412747 0.06918063 0.5091539972133919 1.0879436065541432\n",
      "0.25 0 150\n",
      "0.6222524 0.09697207 0.056640707 0.567543576678246 1.2684985692823845\n",
      "0.25 0 175\n",
      "0.67905694 0.1035797 0.05688446 0.5299110726899308 1.1593082282757095\n",
      "0.25 0 200\n",
      "0.7385131 0.14917725 0.06630196 0.5629858962096856 1.1764000525953104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25 0 225\n",
      "0.7162065 0.14236598 0.07743611 0.5767659030503072 1.2196838770212317\n",
      "0.25 0 250\n",
      "0.647318 0.12221185 0.065250404 0.5529604450479593 1.1661852942793125\n",
      "0.25 0 275\n",
      "0.7748517 0.14183578 0.08381873 0.5968756422139337 1.163538039423589\n",
      "0.25 0 300\n",
      "0.6710045 0.1223993 0.07153831 0.5260033846798361 1.1785643397992849\n",
      "0.25 0 325\n",
      "0.70502883 0.14518121 0.07325665 0.590421805557008 1.26856051094355\n",
      "0.25 0 350\n",
      "0.6642938 0.122583196 0.07409686 0.5173498083875392 1.1329275408442785\n",
      "0.25 0 375\n",
      "0.68450254 0.122221015 0.07653083 0.5657934043466434 1.1728435305774099\n",
      "0.25 0 400\n",
      "0.5882972 0.11621595 0.059905503 0.512833912816514 1.1965067671496192\n",
      "0.25 0 425\n",
      "0.6476149 0.13325807 0.061747607 0.5481885297526371 1.1852802096557402\n",
      "0.25 0 450\n",
      "0.7120792 0.15786698 0.07283409 0.6053131285906173 1.2079669978510819\n",
      "0.25 0 475\n",
      "0.71980053 0.16009563 0.06726426 0.5451415426930764 1.1492674097310263\n",
      "Building model with sigmoid activation and 0.375 dropout\n",
      "0.375 0 0\n",
      "WARNING:tensorflow:Layer flatten_68 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "3.7053137e-06 8.713572e-09 8.573501e-09 1.1429196794845073 1.8950004398363\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 101us/sample - loss: 0.9516 - mse: 0.9516\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.7534 - mse: 0.75340s - loss: 0.7482 - mse: 0.74\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 88us/sample - loss: 0.7341 - mse: 0.7341\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 2s 127us/sample - loss: 0.7116 - mse: 0.7116\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 89us/sample - loss: 0.7204 - mse: 0.7204\n",
      "0.375 0 5\n",
      "0.1770714 0.0013034177 0.00028940718 0.6202304568980994 1.3975010698552357\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.7048 - mse: 0.7048\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.6883 - mse: 0.6883\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.6708 - mse: 0.6708\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.6530 - mse: 0.6530\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 64us/sample - loss: 0.6626 - mse: 0.6626\n",
      "0.375 0 10\n",
      "0.27990475 0.009821554 0.0007742316 0.5362108057605892 1.2297188214062393\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 69us/sample - loss: 0.6414 - mse: 0.6414\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.6363 - mse: 0.6363\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 61us/sample - loss: 0.6274 - mse: 0.6274\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.6266 - mse: 0.6266\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.6208 - mse: 0.6208\n",
      "0.375 0 15\n",
      "0.2977464 0.018760558 0.0030788276 0.5374390342813851 1.2400997826243247\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.6204 - mse: 0.6204\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.6183 - mse: 0.6183\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 65us/sample - loss: 0.6075 - mse: 0.6075\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 67us/sample - loss: 0.6065 - mse: 0.6065\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.5990 - mse: 0.5990\n",
      "0.375 0 20\n",
      "0.33218434 0.02851267 0.0043084384 0.5414799393231086 1.2372098997298102\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 62us/sample - loss: 0.5939 - mse: 0.5939\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 62us/sample - loss: 0.5894 - mse: 0.5894\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.5759 - mse: 0.5759\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.5776 - mse: 0.5776\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.5630 - mse: 0.5630\n",
      "0.375 0 25\n",
      "0.43900692 0.04274688 0.0068952236 0.5293128119998485 1.243627936009095\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 96us/sample - loss: 0.5628 - mse: 0.5628\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 62us/sample - loss: 0.5756 - mse: 0.5756\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.5556 - mse: 0.5556\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.5439 - mse: 0.5439\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 67us/sample - loss: 0.5335 - mse: 0.5335\n",
      "0.375 0 30\n",
      "0.57033104 0.056593835 0.009946389 0.598151954871508 1.35137695030278\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 63us/sample - loss: 0.5370 - mse: 0.5370\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 88us/sample - loss: 0.5275 - mse: 0.5275\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.5369 - mse: 0.5369\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.5247 - mse: 0.5247\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.5217 - mse: 0.5217\n",
      "0.375 0 35\n",
      "0.56040925 0.049917337 0.012597485 0.6545006304903501 1.4085231169462085\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.5202 - mse: 0.5202\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.5259 - mse: 0.5259\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.5258 - mse: 0.5258\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 71us/sample - loss: 0.5075 - mse: 0.5075\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.5211 - mse: 0.5211\n",
      "0.375 0 40\n",
      "0.6108911 0.05537839 0.01354842 0.6727190041198052 1.37595059047618\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.5190 - mse: 0.5190\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.5014 - mse: 0.5014\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.5143 - mse: 0.5143\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.5109 - mse: 0.5109\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.4952 - mse: 0.4952\n",
      "0.375 0 45\n",
      "0.54455966 0.058724273 0.013395324 0.7352455425062909 1.5625396065474682\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 63us/sample - loss: 0.5053 - mse: 0.5053\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 66us/sample - loss: 0.4976 - mse: 0.4976\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.5028 - mse: 0.5028\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.4912 - mse: 0.4912\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.4990 - mse: 0.4990\n",
      "0.375 0 50\n",
      "0.51600665 0.06337572 0.015234873 0.7586270710404964 1.6024669990787792\n",
      "0.375 0 75\n",
      "0.61887795 0.07923596 0.018565996 0.7874459854606067 1.6539206981717347\n",
      "0.375 0 100\n",
      "0.5160814 0.07799124 0.024804523 0.8665482070905034 1.853549726417308\n",
      "0.375 0 125\n",
      "0.5872795 0.10078955 0.031282987 1.0373317791477896 1.9794872885396952\n",
      "0.375 0 150\n",
      "0.5498829 0.093017936 0.036740024 0.9084618497481176 1.8709373252738224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375 0 175\n",
      "0.58108896 0.10912955 0.043317452 0.9950690873977954 1.9413095610038913\n",
      "0.375 0 200\n",
      "0.65504247 0.12302247 0.052638713 1.0389270705782674 1.9645719152031949\n",
      "0.375 0 225\n",
      "0.5865352 0.117578566 0.05701015 1.035515600380847 1.9941399280242076\n",
      "0.375 0 250\n",
      "0.5459232 0.102074794 0.04434771 0.96150763072686 1.9031615889737254\n",
      "0.375 0 275\n",
      "0.49982017 0.09268932 0.046118017 0.9169108147627301 1.8489324254453212\n",
      "0.375 0 300\n",
      "0.5053896 0.104945466 0.05256628 0.997737643885066 2.008547710700522\n",
      "0.375 0 325\n",
      "0.55206734 0.11490978 0.047800686 0.9548324748701443 1.7996023275254853\n",
      "0.375 0 350\n",
      "0.65254605 0.11576554 0.06463111 0.9694567695232302 1.8561179790134572\n",
      "0.375 0 375\n",
      "0.5760461 0.11925466 0.054137234 0.9759659485010046 1.9398692127691914\n",
      "0.375 0 400\n",
      "0.6290839 0.13289982 0.061544575 1.0321323530845072 1.9373290633551479\n",
      "0.375 0 425\n",
      "0.67382884 0.1560358 0.06021769 1.1188865156322227 1.9610592807360885\n",
      "0.375 0 450\n",
      "0.59762615 0.14392854 0.06753017 1.0317700337946094 1.9870014954512063\n",
      "0.375 0 475\n",
      "0.5488486 0.13805301 0.049039442 1.0027462673214373 1.9183518678093752\n",
      "Building model with sigmoid activation and 0.500 dropout\n",
      "0.5 0 0\n",
      "WARNING:tensorflow:Layer flatten_69 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "3.4052662e-06 8.552885e-09 2.9701541e-09 1.09152608127786 2.5706154728357498\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 78us/sample - loss: 1.1095 - mse: 1.1095\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.8264 - mse: 0.8264\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.7866 - mse: 0.7866\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 62us/sample - loss: 0.7751 - mse: 0.7751\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 61us/sample - loss: 0.7684 - mse: 0.7684\n",
      "0.5 0 5\n",
      "0.11259582 0.001123754 0.00021987788 0.6641800424104196 1.4859048044769143\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 63us/sample - loss: 0.7664 - mse: 0.7664\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.7497 - mse: 0.7497\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 65us/sample - loss: 0.7541 - mse: 0.7541\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.7398 - mse: 0.7398\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.7324 - mse: 0.7324\n",
      "0.5 0 10\n",
      "0.20196955 0.0021803924 0.00044063735 0.60353296548475 1.360679484855249\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.7261 - mse: 0.7261\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.7224 - mse: 0.7224\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.7193 - mse: 0.7193\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.7119 - mse: 0.7119\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.7066 - mse: 0.7066\n",
      "0.5 0 15\n",
      "0.24105003 0.008552796 0.0008155054 0.5855036792142378 1.2882442387205273\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.6962 - mse: 0.6962\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.6982 - mse: 0.6982\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.6852 - mse: 0.6852\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.6862 - mse: 0.6862\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.6844 - mse: 0.6844\n",
      "0.5 0 20\n",
      "0.19499892 0.019661153 0.0012058567 0.6139024235980034 1.3412535198409588\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.6915 - mse: 0.6915\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.6922 - mse: 0.6922\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.6898 - mse: 0.6898\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 66us/sample - loss: 0.6773 - mse: 0.6773\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6790 - mse: 0.6790\n",
      "0.5 0 25\n",
      "0.29325578 0.028817615 0.0020999508 0.5739321810757719 1.2701021565102961\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 68us/sample - loss: 0.6781 - mse: 0.6781\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 70us/sample - loss: 0.6741 - mse: 0.6741\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.6758 - mse: 0.6758\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.6670 - mse: 0.6670\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.6733 - mse: 0.6733\n",
      "0.5 0 30\n",
      "0.23373476 0.031049633 0.0022772967 0.6051591326218707 1.3204456425638276\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.6748 - mse: 0.6748\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.6728 - mse: 0.6728\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.6716 - mse: 0.67160s - loss: 0.6391 - ms\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 89us/sample - loss: 0.6697 - mse: 0.6697\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.6656 - mse: 0.66560s - loss: 0.6660 - mse: 0.666\n",
      "0.5 0 35\n",
      "0.23607408 0.035041723 0.002232521 0.6011548107032841 1.3130193240754024\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.6630 - mse: 0.6630\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.6695 - mse: 0.6695\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.6668 - mse: 0.6668\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.6668 - mse: 0.6668\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 61us/sample - loss: 0.6630 - mse: 0.6630\n",
      "0.5 0 40\n",
      "0.21626262 0.03232138 0.0016899434 0.6255947584396293 1.3507663494287452\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 86us/sample - loss: 0.6533 - mse: 0.6533\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 106us/sample - loss: 0.6537 - mse: 0.6537\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 77us/sample - loss: 0.6570 - mse: 0.65700s - loss: 0.6572 - mse: 0.657\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.6556 - mse: 0.6556\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.6524 - mse: 0.65240s - loss: 0.6549 - mse: \n",
      "0.5 0 45\n",
      "0.25203332 0.030705594 0.0020964637 0.6398884290283288 1.359874576424592\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.6539 - mse: 0.6539\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.6493 - mse: 0.6493\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.6509 - mse: 0.6509\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.6433 - mse: 0.6433\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.6507 - mse: 0.6507\n",
      "0.5 0 50\n",
      "0.22253595 0.028023059 0.002369491 0.6637203701559302 1.4313571431566305\n",
      "0.5 0 75\n",
      "0.2637173 0.04455232 0.0052433386 0.8880758191813196 1.8713749275945464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0 100\n",
      "0.3178136 0.053222984 0.0053688157 1.0396262047841176 2.042436288510358\n",
      "0.5 0 125\n",
      "0.28143913 0.047525857 0.007528303 1.0116881539136426 2.012191200273963\n",
      "0.5 0 150\n",
      "0.3947315 0.07002125 0.011141307 1.1048099803041058 2.133518905179189\n",
      "0.5 0 175\n",
      "0.40674642 0.0559459 0.009952498 1.1803527733809671 2.2912935963218124\n",
      "0.5 0 200\n",
      "0.33658266 0.05750735 0.009063613 1.0965857697132726 2.204674550634292\n",
      "0.5 0 225\n",
      "0.3987556 0.059384055 0.012881907 1.171175094928873 2.315433452963027\n",
      "0.5 0 250\n",
      "0.361295 0.07167074 0.014034998 1.2010870448951048 2.331804116068741\n",
      "0.5 0 275\n",
      "0.33402857 0.06215275 0.009967584 1.2088070224588063 2.37427680046057\n",
      "0.5 0 300\n",
      "0.44286802 0.07801159 0.013847403 1.2125427494882792 2.39265225470027\n",
      "0.5 0 325\n",
      "0.41042778 0.07129063 0.010428802 1.1397949542268735 2.2307550413072064\n",
      "0.5 0 350\n",
      "0.4239903 0.08702142 0.017152973 1.2448205395294485 2.3855179836428215\n",
      "0.5 0 375\n",
      "0.38502306 0.08068896 0.019316142 1.1758924323832691 2.298020294504137\n",
      "0.5 0 400\n",
      "0.42224404 0.078509934 0.018660987 1.2059927658043652 2.282581156481683\n",
      "0.5 0 425\n",
      "0.40321013 0.09868774 0.018715486 1.2142758949166177 2.369838433974412\n",
      "0.5 0 450\n",
      "0.503933 0.108787715 0.022195201 1.2370338025254397 2.340671366010526\n",
      "0.5 0 475\n",
      "0.38072357 0.10003032 0.023055684 1.1797671978354811 2.324002826639974\n",
      "Building model with sigmoid activation and 0.000 dropout\n",
      "0.0 1 0\n",
      "WARNING:tensorflow:Layer flatten_70 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1.2646935e-06 5.102576e-09 1.9123614e-09 1.3071450012314352 3.0614810147761577\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 71us/sample - loss: 0.7074 - mse: 0.7074\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.6073 - mse: 0.6073\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.6086 - mse: 0.6086\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 45us/sample - loss: 0.5982 - mse: 0.5982\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.5782 - mse: 0.5782\n",
      "0.0 1 5\n",
      "0.58280945 0.005050831 0.0009814453 0.556843794300986 1.1709679593322946\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.5337 - mse: 0.5337\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.4609 - mse: 0.4609\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.4149 - mse: 0.4149\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.3705 - mse: 0.3705\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.3137 - mse: 0.3137\n",
      "0.0 1 10\n",
      "1.1177753 0.056205615 0.017267527 0.2786653101363201 0.6576813214680113\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 45us/sample - loss: 0.2396 - mse: 0.2396\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 43us/sample - loss: 0.1727 - mse: 0.1727\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 44us/sample - loss: 0.1477 - mse: 0.1477\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 42us/sample - loss: 0.1310 - mse: 0.1310\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 44us/sample - loss: 0.1233 - mse: 0.1233\n",
      "0.0 1 15\n",
      "0.87243366 0.08700896 0.12337272 0.10927942930062767 0.29757441450143307\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.1089 - mse: 0.1089\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.1018 - mse: 0.1018\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.0983 - mse: 0.0983\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.0974 - mse: 0.0974\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.0909 - mse: 0.0909\n",
      "0.0 1 20\n",
      "1.0326079 0.12778337 0.13488628 0.08863240150416965 0.21137158936672293\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.0883 - mse: 0.08830s - loss: 0.0868 - ms\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.0859 - mse: 0.0859\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.0838 - mse: 0.0838\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.0814 - mse: 0.0814\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 79us/sample - loss: 0.0804 - mse: 0.0804\n",
      "0.0 1 25\n",
      "0.97974104 0.106185734 0.12221188 0.0764647555758589 0.24657858818775047\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.0775 - mse: 0.0775\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.0768 - mse: 0.0768\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.0750 - mse: 0.0750\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 78us/sample - loss: 0.0730 - mse: 0.0730\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 79us/sample - loss: 0.0726 - mse: 0.0726\n",
      "0.0 1 30\n",
      "1.004221 0.124537885 0.10592327 0.06905327682563561 0.23170999952640645\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.0731 - mse: 0.0731\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 62us/sample - loss: 0.0696 - mse: 0.0696\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.0689 - mse: 0.0689\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 93us/sample - loss: 0.0683 - mse: 0.0683\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 105us/sample - loss: 0.0673 - mse: 0.0673s - loss: 0.\n",
      "0.0 1 35\n",
      "0.89855945 0.11073214 0.09290735 0.06033216038095967 0.24982266141856377\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.0653 - mse: 0.0653\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.0646 - mse: 0.0646\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.0641 - mse: 0.0641\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 83us/sample - loss: 0.0634 - mse: 0.0634\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 67us/sample - loss: 0.0637 - mse: 0.0637\n",
      "0.0 1 40\n",
      "0.8472337 0.13711262 0.08198604 0.05975826032770753 0.27242175077265596\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.0628 - mse: 0.0628\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 61us/sample - loss: 0.0644 - mse: 0.0644\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.0619 - mse: 0.0619\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.0615 - mse: 0.0615\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.0589 - mse: 0.0589\n",
      "0.0 1 45\n",
      "0.94546986 0.1581755 0.073746115 0.059584433953365164 0.2432198535264837\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.0590 - mse: 0.0590\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.0593 - mse: 0.0593\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.0592 - mse: 0.0592\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 61us/sample - loss: 0.0596 - mse: 0.0596\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.0589 - mse: 0.0589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1 50\n",
      "0.8873391 0.12700328 0.07015193 0.06476318481782659 0.27213757908717323\n",
      "0.0 1 75\n",
      "0.8051168 0.1529818 0.068502605 0.046543043945035854 0.32597613233974987\n",
      "0.0 1 100\n",
      "0.74560946 0.14050524 0.08313402 0.04095486090914924 0.35294867543791586\n",
      "0.0 1 125\n",
      "0.73404634 0.15657093 0.08917766 0.03572511131900856 0.3855160627709359\n",
      "0.0 1 150\n",
      "0.7815783 0.1623483 0.08147987 0.035099267538463756 0.3512136571334093\n",
      "0.0 1 175\n",
      "0.7799839 0.1610896 0.08202495 0.034765613857499315 0.35114338994254773\n",
      "0.0 1 200\n",
      "0.77543277 0.17043856 0.06975253 0.03284442268268179 0.35165048015682304\n",
      "0.0 1 225\n",
      "0.79417336 0.15731287 0.077721156 0.03726612785525165 0.3363869714064131\n",
      "0.0 1 250\n",
      "0.8888642 0.16768387 0.09058762 0.023677058395310032 0.2578914817857224\n",
      "0.0 1 275\n",
      "1.0450443 0.16839325 0.109901085 0.021892291952014805 0.21240686973954967\n",
      "0.0 1 300\n",
      "0.98263353 0.1801484 0.10191023 0.020634117633913936 0.2357565306732338\n",
      "0.0 1 325\n",
      "1.086854 0.18021709 0.122366145 0.021499901484151804 0.20837755638382893\n",
      "0.0 1 350\n",
      "0.9759167 0.19397986 0.10167147 0.017636189844301955 0.22874302663420437\n",
      "0.0 1 375\n",
      "1.0115224 0.18561016 0.12069323 0.016290561739532954 0.23986061606472728\n",
      "0.0 1 400\n",
      "0.9717435 0.20325081 0.11626792 0.015541696704797834 0.26893056121516173\n",
      "0.0 1 425\n",
      "1.0783281 0.2163846 0.12940532 0.01519401083764636 0.25633306253873517\n",
      "0.0 1 450\n",
      "1.0465232 0.2327056 0.11534575 0.011961206750291255 0.25663595757269375\n",
      "0.0 1 475\n",
      "1.0150852 0.2313204 0.112735264 0.010894861958806977 0.2773880622375976\n",
      "Building model with sigmoid activation and 0.050 dropout\n",
      "0.05 1 0\n",
      "WARNING:tensorflow:Layer flatten_71 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2.0392104e-06 6.941056e-09 4.8753996e-09 1.0158490014052752 2.302875024382388\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 3s 209us/sample - loss: 0.7535 - mse: 0.7535\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 2s 184us/sample - loss: 0.6437 - mse: 0.6437\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 2s 129us/sample - loss: 0.6265 - mse: 0.6265s - loss: 0.6411 - mse:\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.6064 - mse: 0.6064\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 65us/sample - loss: 0.5609 - mse: 0.5609\n",
      "0.05 1 5\n",
      "0.50623417 0.0072061885 0.0007586957 0.49131420729692676 1.0663256729011863\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.4948 - mse: 0.4948\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 70us/sample - loss: 0.4609 - mse: 0.4609\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 66us/sample - loss: 0.4295 - mse: 0.4295\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 62us/sample - loss: 0.3912 - mse: 0.3912\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.3519 - mse: 0.3519\n",
      "0.05 1 10\n",
      "0.75725 0.057479084 0.0152441505 0.2927244361281782 0.7198588594611518\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 95us/sample - loss: 0.3218 - mse: 0.3218\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 63us/sample - loss: 0.2953 - mse: 0.2953\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 67us/sample - loss: 0.2629 - mse: 0.2629\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 86us/sample - loss: 0.2448 - mse: 0.2448\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 63us/sample - loss: 0.2329 - mse: 0.2329\n",
      "0.05 1 15\n",
      "0.843073 0.08412499 0.059867337 0.14974741362794697 0.4037509651645567\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 64us/sample - loss: 0.2284 - mse: 0.2284\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 111us/sample - loss: 0.2189 - mse: 0.2189\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.2049 - mse: 0.2049\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 63us/sample - loss: 0.2095 - mse: 0.2095\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 75us/sample - loss: 0.1990 - mse: 0.1990\n",
      "0.05 1 20\n",
      "0.99601775 0.09407053 0.080299616 0.12158418032880386 0.30575470726818027\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.1951 - mse: 0.1951\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.1977 - mse: 0.1977\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 73us/sample - loss: 0.1903 - mse: 0.1903\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 63us/sample - loss: 0.1929 - mse: 0.1929\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 65us/sample - loss: 0.1911 - mse: 0.1911\n",
      "0.05 1 25\n",
      "1.1020638 0.1289603 0.06635977 0.10637777368753525 0.27281941308952956\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 63us/sample - loss: 0.1810 - mse: 0.1810\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.1767 - mse: 0.1767\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.1758 - mse: 0.1758\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.1765 - mse: 0.1765\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.1719 - mse: 0.1719\n",
      "0.05 1 30\n",
      "0.8511561 0.113935985 0.062320326 0.11187249759304757 0.3751497874638799\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 70us/sample - loss: 0.1660 - mse: 0.1660\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.1673 - mse: 0.1673\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 78us/sample - loss: 0.1698 - mse: 0.1698\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.1640 - mse: 0.1640\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.1620 - mse: 0.1620\n",
      "0.05 1 35\n",
      "0.76060593 0.110267654 0.0565435 0.10687854428785254 0.42320548647174994\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.1691 - mse: 0.1691\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.1616 - mse: 0.1616\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.1631 - mse: 0.16310s - loss: 0.1609 - mse: 0.1\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.1559 - mse: 0.1559\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 61us/sample - loss: 0.1644 - mse: 0.1644\n",
      "0.05 1 40\n",
      "0.6912506 0.096507154 0.042563263 0.10705094894272466 0.46800085590187535\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 69us/sample - loss: 0.1566 - mse: 0.1566\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 68us/sample - loss: 0.1568 - mse: 0.1568\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.1562 - mse: 0.1562\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 66us/sample - loss: 0.1513 - mse: 0.1513\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.1506 - mse: 0.1506\n",
      "0.05 1 45\n",
      "0.7687624 0.103675984 0.06661343 0.09338392490280302 0.407329508275948\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.1525 - mse: 0.1525\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.1518 - mse: 0.1518\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.1519 - mse: 0.1519\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.1473 - mse: 0.1473\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.1445 - mse: 0.1445\n",
      "0.05 1 50\n",
      "0.73547417 0.10728958 0.04857215 0.08943822147637609 0.4636798913900011\n",
      "0.05 1 75\n",
      "0.67513597 0.102569245 0.039916437 0.07742036351081044 0.4864259284358924\n",
      "0.05 1 100\n",
      "0.74306333 0.09330802 0.046668157 0.0687485326652966 0.4376611933152552\n",
      "0.05 1 125\n",
      "0.76330715 0.1004088 0.057955906 0.0695591341918872 0.42050153457312256\n",
      "0.05 1 150\n",
      "0.74170446 0.11343635 0.05144931 0.0796453455045341 0.4213800769187469\n",
      "0.05 1 175\n",
      "0.7072716 0.113618754 0.046902116 0.049848226133918656 0.45265064280547335\n",
      "0.05 1 200\n",
      "0.80091375 0.11384642 0.07650973 0.055744892734469864 0.38515382744135207\n",
      "0.05 1 225\n",
      "0.73966163 0.13747713 0.055225126 0.060418559758932434 0.4842859010763729\n",
      "0.05 1 250\n",
      "0.79915094 0.1352284 0.06427593 0.05273031430678116 0.40748651081987475\n",
      "0.05 1 275\n",
      "0.80317074 0.12627815 0.07060031 0.05445929323431424 0.42521370792458485\n",
      "0.05 1 300\n",
      "0.7545176 0.11118477 0.06598198 0.04261186379633723 0.44192661688596446\n",
      "0.05 1 325\n",
      "0.7063802 0.09701665 0.06822846 0.04226193655438312 0.4429096630448082\n",
      "0.05 1 350\n",
      "0.6779772 0.097936414 0.082700156 0.04459307883272471 0.5239659008015614\n",
      "0.05 1 375\n",
      "0.7737931 0.10436206 0.079920426 0.046111113161758366 0.48297855309921206\n",
      "0.05 1 400\n",
      "0.88200456 0.12789953 0.08615811 0.04242093721183985 0.3642500306029011\n",
      "0.05 1 425\n",
      "0.7365515 0.10554319 0.08421023 0.042118090876080615 0.4761995512851473\n",
      "0.05 1 450\n",
      "0.7957601 0.1203179 0.0631751 0.0384829799393749 0.40874784710227396\n",
      "0.05 1 475\n",
      "0.819877 0.14791228 0.07439733 0.0402850488588014 0.3990529497163623\n",
      "Building model with sigmoid activation and 0.125 dropout\n",
      "0.125 1 0\n",
      "WARNING:tensorflow:Layer flatten_72 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "9.299967e-07 3.4664605e-09 1.3864677e-09 1.0957547076744123 2.586079941944976\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 95us/sample - loss: 0.8002 - mse: 0.8002\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 64us/sample - loss: 0.6707 - mse: 0.6707\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 72us/sample - loss: 0.6496 - mse: 0.6496\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.6350 - mse: 0.6350\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.6090 - mse: 0.6090\n",
      "0.125 1 5\n",
      "0.39302406 0.0041888715 0.0006643696 0.5454502390311937 1.1980319266724506\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 61us/sample - loss: 0.5769 - mse: 0.5769\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.5446 - mse: 0.54460s - loss: 0.5502 - mse: 0\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 72us/sample - loss: 0.5185 - mse: 0.5185\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.4975 - mse: 0.4975\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 68us/sample - loss: 0.4747 - mse: 0.4747\n",
      "0.125 1 10\n",
      "0.85282594 0.056481104 0.0054198024 0.3827533904297526 0.8674373906855514\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.4362 - mse: 0.4362\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.4171 - mse: 0.4171\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.3985 - mse: 0.3985\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.3766 - mse: 0.3766\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.3579 - mse: 0.3579\n",
      "0.125 1 15\n",
      "0.83470047 0.092248194 0.024282802 0.2633213522953403 0.6272058487317015\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.3477 - mse: 0.3477\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 66us/sample - loss: 0.3384 - mse: 0.3384\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 65us/sample - loss: 0.3287 - mse: 0.3287\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.3214 - mse: 0.3214\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 63us/sample - loss: 0.3239 - mse: 0.3239\n",
      "0.125 1 20\n",
      "0.9274554 0.10942744 0.042003814 0.21778507645064313 0.5391466531604612\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 70us/sample - loss: 0.3065 - mse: 0.3065\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 76us/sample - loss: 0.3119 - mse: 0.3119\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 61us/sample - loss: 0.3066 - mse: 0.3066\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 71us/sample - loss: 0.2963 - mse: 0.2963\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.2928 - mse: 0.2928\n",
      "0.125 1 25\n",
      "0.9084831 0.11316127 0.037572175 0.2215811570281192 0.582282999642412\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 70us/sample - loss: 0.2881 - mse: 0.2881\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 67us/sample - loss: 0.2835 - mse: 0.2835\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.2821 - mse: 0.28210s - loss: 0.2897 - mse: \n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.2777 - mse: 0.2777\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.2700 - mse: 0.2700\n",
      "0.125 1 30\n",
      "0.8740038 0.09930575 0.035306655 0.19562533046967748 0.5270344224383529\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.2703 - mse: 0.2703\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.2681 - mse: 0.2681\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 66us/sample - loss: 0.2717 - mse: 0.2717\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.2651 - mse: 0.2651\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.2596 - mse: 0.2596\n",
      "0.125 1 35\n",
      "0.97376937 0.10771033 0.0508333 0.18368105440345445 0.565779168491626\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.2571 - mse: 0.2571\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.2532 - mse: 0.2532\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.2494 - mse: 0.2494\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.2558 - mse: 0.2558\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 64us/sample - loss: 0.2573 - mse: 0.2573\n",
      "0.125 1 40\n",
      "0.992013 0.10531767 0.05407225 0.16858206505119946 0.5149733804342451\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.2447 - mse: 0.2447\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.2452 - mse: 0.2452\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.2505 - mse: 0.25050s - loss: 0.2506 - mse: 0.250\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.2511 - mse: 0.25110s - loss: 0.2450 - m - ETA: 0s - loss: 0.2505 - mse: 0.25\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.2529 - mse: 0.2529\n",
      "0.125 1 45\n",
      "0.89308447 0.10549886 0.04057703 0.1680224221819033 0.5496372634371108\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 71us/sample - loss: 0.2412 - mse: 0.2412\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.2436 - mse: 0.2436\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 65us/sample - loss: 0.2525 - mse: 0.2525\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 62us/sample - loss: 0.2349 - mse: 0.2349\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 70us/sample - loss: 0.2402 - mse: 0.2402\n",
      "0.125 1 50\n",
      "0.82560986 0.10442355 0.059629884 0.19574147224262078 0.6316840393259776\n",
      "0.125 1 75\n",
      "0.81871206 0.10340184 0.05340664 0.16118282192403327 0.5137248626024236\n",
      "0.125 1 100\n",
      "0.90278834 0.13067769 0.060377724 0.1938797647420851 0.513607027270685\n",
      "0.125 1 125\n",
      "0.6340219 0.08795445 0.052299865 0.1714509166418691 0.6177780118198172\n",
      "0.125 1 150\n",
      "0.6743882 0.091067284 0.050862234 0.15133514805706275 0.6261235486357031\n",
      "0.125 1 175\n",
      "0.6978895 0.113065384 0.059510086 0.19071956669183635 0.6624616129556163\n",
      "0.125 1 200\n",
      "0.7667239 0.12986521 0.08220471 0.17134483242020018 0.5847776980151147\n",
      "0.125 1 225\n",
      "0.696757 0.11909501 0.062003106 0.17333329660667568 0.6048541110632024\n",
      "0.125 1 250\n",
      "0.7640076 0.14306512 0.07549676 0.19367484840348698 0.6426556545235615\n",
      "0.125 1 275\n",
      "0.7017739 0.09878987 0.062150862 0.15215221199488563 0.6239381285660455\n",
      "0.125 1 300\n",
      "0.7864219 0.122663535 0.06941743 0.1736940557430945 0.6254223018219917\n",
      "0.125 1 325\n",
      "0.73023814 0.12628713 0.06747897 0.1521903459592485 0.6163249149156836\n",
      "0.125 1 350\n",
      "0.78553545 0.12872946 0.07156332 0.15415720244931175 0.5761570579960625\n",
      "0.125 1 375\n",
      "0.77428675 0.14882953 0.07235567 0.20075962200568495 0.6637480715159735\n",
      "0.125 1 400\n",
      "0.754015 0.14060986 0.06772513 0.18317861549111147 0.612933265247363\n",
      "0.125 1 425\n",
      "0.8438994 0.1264938 0.08194215 0.14065378138706155 0.5356548664561069\n",
      "0.125 1 450\n",
      "0.7381593 0.13652934 0.07233933 0.16420388072353828 0.6121005364339681\n",
      "0.125 1 475\n",
      "0.6938569 0.13044359 0.06387424 0.1577346811409208 0.6054032514631149\n",
      "Building model with sigmoid activation and 0.250 dropout\n",
      "0.25 1 0\n",
      "WARNING:tensorflow:Layer flatten_73 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "3.8415455e-06 8.294273e-09 4.0825494e-09 2.1967685315615526 2.177299401520406\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 92us/sample - loss: 0.9112 - mse: 0.9112\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 63us/sample - loss: 0.7055 - mse: 0.7055\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.6882 - mse: 0.6882\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.6692 - mse: 0.6692\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.6601 - mse: 0.6601\n",
      "0.25 1 5\n",
      "0.3956238 0.0045106607 0.00095996604 0.5582857252625018 1.2264684738709184\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 66us/sample - loss: 0.6490 - mse: 0.6490\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.6284 - mse: 0.6284\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.6013 - mse: 0.6013\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 67us/sample - loss: 0.5879 - mse: 0.5879\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 62us/sample - loss: 0.5677 - mse: 0.56770s - loss: 0.5678 - mse: 0.567\n",
      "0.25 1 10\n",
      "0.47131833 0.022714444 0.0026285746 0.48203756209960236 1.095949470669252\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 85us/sample - loss: 0.5687 - mse: 0.5687\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 67us/sample - loss: 0.5615 - mse: 0.5615\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.5506 - mse: 0.5506\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.5454 - mse: 0.5454\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.5500 - mse: 0.5500\n",
      "0.25 1 15\n",
      "0.49049437 0.036439627 0.0058518318 0.45287850858674694 1.0632481272734036\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.5344 - mse: 0.5344\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.5286 - mse: 0.5286\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.4996 - mse: 0.4996\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.4916 - mse: 0.4916\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.4697 - mse: 0.4697\n",
      "0.25 1 20\n",
      "0.6173397 0.049097475 0.013094679 0.39730798022914465 0.9682924440756491\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.4609 - mse: 0.4609\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.4524 - mse: 0.4524\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 62us/sample - loss: 0.4451 - mse: 0.4451\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.4436 - mse: 0.4436\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.4325 - mse: 0.4325\n",
      "0.25 1 25\n",
      "0.61901253 0.06254822 0.01875706 0.40539989839635576 1.02245243547723\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.4307 - mse: 0.4307\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.4214 - mse: 0.4214\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.4232 - mse: 0.4232\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.4083 - mse: 0.4083\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 66us/sample - loss: 0.4118 - mse: 0.4118\n",
      "0.25 1 30\n",
      "0.76191795 0.103569865 0.03409717 0.4984132655195385 1.2329370079663402\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.3998 - mse: 0.3998\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.4102 - mse: 0.4102\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.3943 - mse: 0.3943\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 64us/sample - loss: 0.3991 - mse: 0.3991\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.3921 - mse: 0.3921\n",
      "0.25 1 35\n",
      "0.7277091 0.09860813 0.03173349 0.3944885816990554 1.0523581866102607\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.3909 - mse: 0.3909\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.3905 - mse: 0.3905\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.3886 - mse: 0.3886\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.3904 - mse: 0.3904\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.3787 - mse: 0.3787\n",
      "0.25 1 40\n",
      "0.81903535 0.100078925 0.035759013 0.5059828284641524 1.1669231892984266\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.3756 - mse: 0.3756\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.3765 - mse: 0.3765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 63us/sample - loss: 0.3729 - mse: 0.3729\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.3877 - mse: 0.3877\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.3676 - mse: 0.3676\n",
      "0.25 1 45\n",
      "0.6956607 0.08153936 0.028709987 0.43871550963221845 1.13621285702809\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.3617 - mse: 0.3617\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.3731 - mse: 0.3731\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.3753 - mse: 0.3753\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.3656 - mse: 0.3656\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 62us/sample - loss: 0.3671 - mse: 0.3671\n",
      "0.25 1 50\n",
      "0.648812 0.08235985 0.02404262 0.47531904088533383 1.2324392250674754\n",
      "0.25 1 75\n",
      "0.7013966 0.105367206 0.04018886 0.5633212811431503 1.2766788553869106\n",
      "0.25 1 100\n",
      "0.66936255 0.10102776 0.04198065 0.5114006300536316 1.1518303589233951\n",
      "0.25 1 125\n",
      "0.64443713 0.12047524 0.050479684 0.5288803467657083 1.1895843576988958\n",
      "0.25 1 150\n",
      "0.6768066 0.11123619 0.052516177 0.5443707763911947 1.1849991968437947\n",
      "0.25 1 175\n",
      "0.7039677 0.12670396 0.07058886 0.6000956787538991 1.2636162761449925\n",
      "0.25 1 200\n",
      "0.702547 0.12431807 0.06694341 0.5486580349360557 1.1989867492485928\n",
      "0.25 1 225\n",
      "0.6753415 0.11021352 0.07909952 0.6000296216250518 1.2817532255457786\n",
      "0.25 1 250\n",
      "0.7320677 0.12940824 0.07154688 0.6530596306596111 1.340189222623597\n",
      "0.25 1 275\n",
      "0.63561183 0.12700257 0.06691865 0.5524826154453821 1.28188285116992\n",
      "0.25 1 300\n",
      "0.5998915 0.12217933 0.069651276 0.5907543169329758 1.2530631196473234\n",
      "0.25 1 325\n",
      "0.60000247 0.11376099 0.056160096 0.5721652583061944 1.2876009661654804\n",
      "0.25 1 350\n",
      "0.71138084 0.14341354 0.07863342 0.5530685069767419 1.1248214069249047\n",
      "0.25 1 375\n",
      "0.72294205 0.13592917 0.07585027 0.5894383969339277 1.1689576384838665\n",
      "0.25 1 400\n",
      "0.6890791 0.10160681 0.07769498 0.5402283702345777 1.1934827242307395\n",
      "0.25 1 425\n",
      "0.68865925 0.13041691 0.08081242 0.6084609779048765 1.2664158052141943\n",
      "0.25 1 450\n",
      "0.69638973 0.12948865 0.07883694 0.5673316748772366 1.2194800149760374\n",
      "0.25 1 475\n",
      "0.6435722 0.123726144 0.071149096 0.54681543498124 1.16276064705834\n",
      "Building model with sigmoid activation and 0.375 dropout\n",
      "0.375 1 0\n",
      "WARNING:tensorflow:Layer flatten_74 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1.7263063e-06 6.7500374e-09 3.381499e-09 1.1895022458054285 1.8757939197696143\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 85us/sample - loss: 0.9513 - mse: 0.9513\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.7486 - mse: 0.7486\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 67us/sample - loss: 0.7282 - mse: 0.7282\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.7206 - mse: 0.72060s - loss: 0.7213 - mse: 0.721\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.7136 - mse: 0.7136\n",
      "0.375 1 5\n",
      "0.21948065 0.0017457716 0.00037518065 0.6054960833597123 1.355303252278015\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.6992 - mse: 0.6992\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6916 - mse: 0.6916\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.6760 - mse: 0.6760\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.6720 - mse: 0.6720\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.6545 - mse: 0.6545\n",
      "0.375 1 10\n",
      "0.3049733 0.009312636 0.0008750644 0.542357539452305 1.2335887580530247\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.6525 - mse: 0.6525\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6407 - mse: 0.6407\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6325 - mse: 0.6325\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.6324 - mse: 0.6324\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.6207 - mse: 0.6207\n",
      "0.375 1 15\n",
      "0.3860984 0.02961064 0.0026928936 0.5334670284220332 1.173096923453905\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.6194 - mse: 0.61940s - loss: 0.6287 - mse:\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.6126 - mse: 0.6126\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.6003 - mse: 0.6003\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.6180 - mse: 0.6180\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.6070 - mse: 0.6070\n",
      "0.375 1 20\n",
      "0.4046398 0.03845696 0.004863681 0.5108433531514568 1.1692324061411843\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.5920 - mse: 0.5920\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.6082 - mse: 0.6082\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.5901 - mse: 0.5901\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.5890 - mse: 0.5890\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.5900 - mse: 0.5900\n",
      "0.375 1 25\n",
      "0.3603132 0.036719333 0.00750318 0.5198629323351157 1.2496643602495614\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.5774 - mse: 0.5774\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.5631 - mse: 0.5631\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.5536 - mse: 0.5536\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.5464 - mse: 0.5464\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.5517 - mse: 0.5517\n",
      "0.375 1 30\n",
      "0.4252189 0.04827648 0.0103598405 0.6073434171795239 1.3611415826831617\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.5397 - mse: 0.5397\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.5444 - mse: 0.5444\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.5304 - mse: 0.5304\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.5392 - mse: 0.5392\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.5253 - mse: 0.5253\n",
      "0.375 1 35\n",
      "0.42644885 0.042844642 0.011428159 0.6225905929892009 1.4542415765403043\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.5238 - mse: 0.5238\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.5293 - mse: 0.5293\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.5143 - mse: 0.5143\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.5080 - mse: 0.5080\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.5161 - mse: 0.5161\n",
      "0.375 1 40\n",
      "0.4839656 0.055013705 0.016133102 0.7156501414567965 1.672779388981362\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13003/13003 [==============================] - 1s 63us/sample - loss: 0.5077 - mse: 0.5077\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.5130 - mse: 0.5130\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.4988 - mse: 0.4988\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.4843 - mse: 0.4843\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.5047 - mse: 0.5047\n",
      "0.375 1 45\n",
      "0.49240395 0.05605495 0.014223962 0.6206147459033018 1.4146039754671023\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.5020 - mse: 0.5020\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.4952 - mse: 0.4952\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 72us/sample - loss: 0.4863 - mse: 0.4863\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 67us/sample - loss: 0.4810 - mse: 0.4810\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 75us/sample - loss: 0.4951 - mse: 0.4951\n",
      "0.375 1 50\n",
      "0.54204714 0.07988554 0.020469377 0.7895803950302221 1.753603156815596\n",
      "0.375 1 75\n",
      "0.5282038 0.07070434 0.01982149 0.8066127887143291 1.678148515414969\n",
      "0.375 1 100\n",
      "0.5614583 0.08023377 0.027791489 0.893022851762506 1.9170784518333635\n",
      "0.375 1 125\n",
      "0.5449454 0.09223816 0.03207006 0.9102821171193393 1.8716288297905783\n",
      "0.375 1 150\n",
      "0.54140574 0.10254293 0.03935101 0.9303251933891772 1.8391561092919841\n",
      "0.375 1 175\n",
      "0.571829 0.11374138 0.04871634 0.9750139508258729 1.934472921652183\n",
      "0.375 1 200\n",
      "0.73749 0.1391914 0.056130487 1.154876589362528 2.034820545889838\n",
      "0.375 1 225\n",
      "0.5721178 0.09906547 0.057058964 0.9533536293601985 1.9056076344722197\n",
      "0.375 1 250\n",
      "0.57368016 0.11743142 0.047637988 0.9429984352184274 1.8802757577720486\n",
      "0.375 1 275\n",
      "0.5823352 0.109573126 0.058527846 1.035008345570591 1.9858878449797879\n",
      "0.375 1 300\n",
      "0.6115495 0.13229237 0.050882522 1.016769917521809 1.9066526189187012\n",
      "0.375 1 325\n",
      "0.58280385 0.12102316 0.056265254 1.036420407807631 1.9812952784502313\n",
      "0.375 1 350\n",
      "0.64016277 0.12173967 0.06637541 1.0804972143542544 1.9935683000948288\n",
      "0.375 1 375\n",
      "0.56389225 0.11997001 0.061279703 1.0241993346989453 1.9557657333473148\n",
      "0.375 1 400\n",
      "0.5480472 0.10493311 0.061124094 0.9846106186306294 1.89470567924402\n",
      "0.375 1 425\n",
      "0.5663133 0.12736546 0.067867346 0.9766807230865523 1.8925831352099867\n",
      "0.375 1 450\n",
      "0.547607 0.121522725 0.0643073 1.0050101944541472 1.9589371000468105\n",
      "0.375 1 475\n",
      "0.6341276 0.14838986 0.07216255 1.0874710883943566 2.0763012493108772\n",
      "Building model with sigmoid activation and 0.500 dropout\n",
      "0.5 1 0\n",
      "WARNING:tensorflow:Layer flatten_75 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1.339425e-06 6.063478e-09 2.5961642e-09 1.1950081386292484 2.8228104561903464\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 88us/sample - loss: 1.1197 - mse: 1.1197\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.8270 - mse: 0.8270\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.7894 - mse: 0.7894\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 87us/sample - loss: 0.7771 - mse: 0.7771\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 99us/sample - loss: 0.7592 - mse: 0.75920s - loss: 0.7597 - mse: 0.7\n",
      "0.5 1 5\n",
      "0.14136513 0.0013922873 0.0003189978 0.6420152717408582 1.43114997771024\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 62us/sample - loss: 0.7565 - mse: 0.7565\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.7535 - mse: 0.7535\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.7472 - mse: 0.7472\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.7381 - mse: 0.7381\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.7296 - mse: 0.7296\n",
      "0.5 1 10\n",
      "0.20047262 0.0033069428 0.0005071012 0.6089991199478885 1.3477620902327736\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.7380 - mse: 0.7380\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.7209 - mse: 0.7209\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.7238 - mse: 0.7238\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.7098 - mse: 0.7098\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.7076 - mse: 0.7076\n",
      "0.5 1 15\n",
      "0.21792756 0.008137184 0.00068952056 0.5814699136640971 1.309781861804371\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.7005 - mse: 0.7005\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6951 - mse: 0.6951\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 67us/sample - loss: 0.6818 - mse: 0.68180s - loss: 0.661\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 71us/sample - loss: 0.6921 - mse: 0.6921\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.6848 - mse: 0.6848\n",
      "0.5 1 20\n",
      "0.26938006 0.023756946 0.0016709382 0.5793462929606144 1.280454597472777\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6890 - mse: 0.6890\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.6872 - mse: 0.6872\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.6930 - mse: 0.6930\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.6857 - mse: 0.6857\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 70us/sample - loss: 0.6748 - mse: 0.6748\n",
      "0.5 1 25\n",
      "0.21980928 0.02400537 0.0018234836 0.6084289444443943 1.3212580106627134\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 64us/sample - loss: 0.6812 - mse: 0.6812\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.6793 - mse: 0.6793\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.6729 - mse: 0.6729\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.6656 - mse: 0.6656\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6703 - mse: 0.6703\n",
      "0.5 1 30\n",
      "0.23123476 0.027025972 0.0024179898 0.6008878113776142 1.314901709300701\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.6656 - mse: 0.6656\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.6678 - mse: 0.6678\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.6729 - mse: 0.6729\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.6648 - mse: 0.6648\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.6663 - mse: 0.6663\n",
      "0.5 1 35\n",
      "0.27342668 0.03487416 0.0025029397 0.6041210417899553 1.2961564658139633\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.6636 - mse: 0.6636\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.6616 - mse: 0.6616\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.6707 - mse: 0.6707\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.6589 - mse: 0.6589\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.6535 - mse: 0.6535\n",
      "0.5 1 40\n",
      "0.32390028 0.042126015 0.0032149036 0.6137349694233623 1.3064948025554937\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.6515 - mse: 0.6515\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.6621 - mse: 0.6621\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.6546 - mse: 0.6546\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.6517 - mse: 0.6517\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.6528 - mse: 0.6528\n",
      "0.5 1 45\n",
      "0.26711798 0.032561656 0.003251008 0.6254278494652935 1.3552241655241606\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.6540 - mse: 0.6540\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.6465 - mse: 0.6465\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.6428 - mse: 0.6428\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.6403 - mse: 0.6403\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.6469 - mse: 0.6469\n",
      "0.5 1 50\n",
      "0.2648643 0.034068845 0.0027804335 0.6583644357871818 1.4245174695031944\n",
      "0.5 1 75\n",
      "0.3440423 0.056721885 0.0043933904 0.9019300077520743 1.7212013623646183\n",
      "0.5 1 100\n",
      "0.2991351 0.064675495 0.0066359886 0.9939938358059354 1.9489710192713339\n",
      "0.5 1 125\n",
      "0.36944297 0.051367026 0.007209771 0.9946374440303108 2.027670702457562\n",
      "0.5 1 150\n",
      "0.37674686 0.06376899 0.009584712 1.1459671105781037 2.157590373656897\n",
      "0.5 1 175\n",
      "0.35601807 0.051047582 0.009763706 1.057398331483864 2.121875496074184\n",
      "0.5 1 200\n",
      "0.34742397 0.0588498 0.011360267 1.151064972200723 2.2940647808032972\n",
      "0.5 1 225\n",
      "0.38240334 0.06460854 0.009335754 1.1287568930314709 2.175360926516169\n",
      "0.5 1 250\n",
      "0.41331428 0.077895336 0.0130277155 1.2318573804891222 2.3383935952850963\n",
      "0.5 1 275\n",
      "0.39469516 0.075178035 0.013653858 1.2878205649590175 2.4332198800779876\n",
      "0.5 1 300\n",
      "0.39617452 0.07617415 0.016382609 1.2134852312347835 2.308111969125057\n",
      "0.5 1 325\n",
      "0.42852196 0.08581472 0.02025932 1.2031985162231185 2.302130013915233\n",
      "0.5 1 350\n",
      "0.43207315 0.08559663 0.02151846 1.1853555344455156 2.276288696240021\n",
      "0.5 1 375\n",
      "0.43753296 0.09066301 0.023801357 1.2233438218376629 2.355907515633132\n",
      "0.5 1 400\n",
      "0.42354432 0.07886718 0.017957732 1.1424721287029886 2.280861752241646\n",
      "0.5 1 425\n",
      "0.41757387 0.09259995 0.01922877 1.1808867436916808 2.3199334392148354\n",
      "0.5 1 450\n",
      "0.4020625 0.074476704 0.020587021 1.1637680095139462 2.3351976756433936\n",
      "0.5 1 475\n",
      "0.46995524 0.09602865 0.021681935 1.2393493400081839 2.371193901571257\n",
      "Building model with sigmoid activation and 0.000 dropout\n",
      "0.0 2 0\n",
      "WARNING:tensorflow:Layer flatten_76 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1.537234e-06 8.5742515e-09 5.1205933e-09 1.8189887631999997 3.947881885933508\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 89us/sample - loss: 0.7346 - mse: 0.7346\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.6101 - mse: 0.6101\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6041 - mse: 0.6041\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 45us/sample - loss: 0.5835 - mse: 0.5835\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 45us/sample - loss: 0.5476 - mse: 0.5476\n",
      "0.0 2 5\n",
      "0.56944144 0.0074672997 0.00087586744 0.5153697859061359 1.0840343847281293\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.4790 - mse: 0.4790\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.4302 - mse: 0.4302\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.3743 - mse: 0.3743\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.3126 - mse: 0.3126\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.2392 - mse: 0.2392\n",
      "0.0 2 10\n",
      "0.82586414 0.10769094 0.041520424 0.1924935488602031 0.49280977682158356\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.1829 - mse: 0.1829\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 71us/sample - loss: 0.1601 - mse: 0.1601\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.1424 - mse: 0.1424\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.1350 - mse: 0.1350\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.1283 - mse: 0.1283\n",
      "0.0 2 15\n",
      "1.1270447 0.12200312 0.09572035 0.11575411558151114 0.296830919921288\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.1190 - mse: 0.1190\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.1122 - mse: 0.1122\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.1082 - mse: 0.10820s - loss: 0.1127 - mse:\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.1034 - mse: 0.1034\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.0979 - mse: 0.0979\n",
      "0.0 2 20\n",
      "1.1247617 0.12130478 0.13947622 0.08676411357051524 0.22715160327891734\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.0922 - mse: 0.0922\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.0871 - mse: 0.0871\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.0860 - mse: 0.0860\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.0820 - mse: 0.0820\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.0811 - mse: 0.0811\n",
      "0.0 2 25\n",
      "1.0776094 0.12430566 0.1269649 0.07176633107285449 0.19643152194222493\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 63us/sample - loss: 0.0832 - mse: 0.0832\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 62us/sample - loss: 0.0765 - mse: 0.0765\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 69us/sample - loss: 0.0751 - mse: 0.0751\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.0726 - mse: 0.0726\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.0735 - mse: 0.0735\n",
      "0.0 2 30\n",
      "1.0807791 0.18215027 0.07743982 0.07136068893521069 0.22501538853027503\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.0730 - mse: 0.0730\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.0722 - mse: 0.0722\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 45us/sample - loss: 0.0679 - mse: 0.0679\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.0664 - mse: 0.0664\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.0662 - mse: 0.0662\n",
      "0.0 2 35\n",
      "0.93108785 0.15266863 0.058199786 0.06269887942744617 0.25332572028134687\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 63us/sample - loss: 0.0667 - mse: 0.0667\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.0657 - mse: 0.0657\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.0651 - mse: 0.0651\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.0657 - mse: 0.0657\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.0654 - mse: 0.0654\n",
      "0.0 2 40\n",
      "1.00577 0.16383311 0.07212142 0.07420632901489774 0.22412673319901913\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.0624 - mse: 0.0624\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.0639 - mse: 0.0639\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.0615 - mse: 0.0615\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.0620 - mse: 0.0620\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 45us/sample - loss: 0.0620 - mse: 0.0620\n",
      "0.0 2 45\n",
      "0.8559191 0.18411006 0.047311287 0.05919696568826775 0.25910245430674117\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 44us/sample - loss: 0.0630 - mse: 0.0630\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 45us/sample - loss: 0.0614 - mse: 0.0614\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.0614 - mse: 0.0614\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 45us/sample - loss: 0.0608 - mse: 0.0608\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.0598 - mse: 0.0598\n",
      "0.0 2 50\n",
      "0.9299587 0.15634069 0.06680699 0.07933124156817266 0.22747348499825756\n",
      "0.0 2 75\n",
      "0.9349039 0.16763929 0.06857909 0.04787145593429894 0.27419656207690285\n",
      "0.0 2 100\n",
      "0.92280513 0.1782992 0.052088793 0.04327151387533839 0.25835668921834337\n",
      "0.0 2 125\n",
      "0.9508163 0.18158841 0.084227465 0.05335446290784257 0.2452512791824328\n",
      "0.0 2 150\n",
      "0.96166927 0.17633517 0.0841237 0.04080872812852202 0.2362864916640632\n",
      "0.0 2 175\n",
      "0.9208769 0.16239859 0.09510007 0.03488998938091729 0.2529809281751284\n",
      "0.0 2 200\n",
      "0.9453661 0.16725427 0.094222814 0.03364744567146091 0.23961667292281488\n",
      "0.0 2 225\n",
      "0.8225006 0.13125493 0.08822139 0.02968816945993244 0.3071761882791663\n",
      "0.0 2 250\n",
      "0.93577087 0.12459334 0.10893156 0.02482529888861702 0.24476085859500582\n",
      "0.0 2 275\n",
      "0.9155047 0.14283623 0.098579235 0.023565719196618016 0.2629928550418244\n",
      "0.0 2 300\n",
      "0.9659457 0.13989398 0.10482095 0.02197490963121983 0.24966891075962008\n",
      "0.0 2 325\n",
      "0.9651273 0.15251869 0.105290666 0.02071304358715301 0.26969500580081157\n",
      "0.0 2 350\n",
      "0.9532221 0.13728614 0.10102978 0.018578819579224186 0.28185831304872017\n",
      "0.0 2 375\n",
      "0.9194683 0.1430072 0.09897656 0.01732661374916467 0.3040931852267708\n",
      "0.0 2 400\n",
      "1.0176823 0.1669128 0.09791939 0.01570226775812821 0.29205054240710593\n",
      "0.0 2 425\n",
      "0.9798249 0.15589581 0.105707094 0.014776499237426273 0.29013017541315245\n",
      "0.0 2 450\n",
      "1.0135722 0.16630065 0.116182275 0.013739783391372926 0.30273807295184535\n",
      "0.0 2 475\n",
      "1.0381602 0.15829988 0.12492529 0.013751681648357857 0.3008393873744743\n",
      "Building model with sigmoid activation and 0.050 dropout\n",
      "0.05 2 0\n",
      "WARNING:tensorflow:Layer flatten_77 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2.1243843e-06 1.4624961e-08 6.134097e-09 0.999494577529571 2.170220073725945\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 86us/sample - loss: 0.7392 - mse: 0.73920s - loss: 0.8334 - mse\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.6408 - mse: 0.6408\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.6265 - mse: 0.6265\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.6172 - mse: 0.6172\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6012 - mse: 0.6012\n",
      "0.05 2 5\n",
      "0.50126284 0.004279728 0.0009039683 0.5558875016527847 1.2338928376377927\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.5646 - mse: 0.5646\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.5006 - mse: 0.5006\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 63us/sample - loss: 0.4707 - mse: 0.47070s - loss: 0.4781 - mse: 0.4\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.4401 - mse: 0.4401\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.3875 - mse: 0.3875\n",
      "0.05 2 10\n",
      "0.92432696 0.048195437 0.010591252 0.3097904203370395 0.7118381692836433\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.3496 - mse: 0.3496\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.3055 - mse: 0.3055\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.2759 - mse: 0.2759\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.2496 - mse: 0.2496\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 61us/sample - loss: 0.2398 - mse: 0.2398\n",
      "0.05 2 15\n",
      "1.0612983 0.12221086 0.062989116 0.166055386739259 0.42304011737542613\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.2376 - mse: 0.2376\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 84us/sample - loss: 0.2207 - mse: 0.2207\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 112us/sample - loss: 0.2223 - mse: 0.2223\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.2005 - mse: 0.2005\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.2019 - mse: 0.2019\n",
      "0.05 2 20\n",
      "0.87817925 0.11388262 0.052407056 0.12298662784230403 0.3465678153534486\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.2062 - mse: 0.2062\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.2003 - mse: 0.2003\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.1881 - mse: 0.1881\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.1845 - mse: 0.1845\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.1816 - mse: 0.1816\n",
      "0.05 2 25\n",
      "0.9389093 0.13337451 0.05143589 0.1095504454042082 0.3284381271928113\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.1866 - mse: 0.1866\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.1888 - mse: 0.1888\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.1783 - mse: 0.1783\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.1750 - mse: 0.1750\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.1769 - mse: 0.1769\n",
      "0.05 2 30\n",
      "0.9683546 0.14009526 0.076470554 0.11089932795370042 0.347176130066301\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 64us/sample - loss: 0.1777 - mse: 0.1777\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.1655 - mse: 0.1655\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.1640 - mse: 0.1640\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.1576 - mse: 0.1576\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.1618 - mse: 0.1618\n",
      "0.05 2 35\n",
      "0.9851842 0.12067936 0.06770294 0.0915520882948812 0.31344206336510916\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.1622 - mse: 0.1622\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.1592 - mse: 0.1592\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.1631 - mse: 0.1631\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.1505 - mse: 0.1505\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.1619 - mse: 0.1619\n",
      "0.05 2 40\n",
      "0.9596597 0.12287791 0.07009942 0.10218503786319924 0.3460709509313739\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.1520 - mse: 0.1520\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.1496 - mse: 0.1496\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.1556 - mse: 0.1556\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.1499 - mse: 0.1499\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.1510 - mse: 0.1510\n",
      "0.05 2 45\n",
      "0.8455222 0.10212695 0.063811906 0.09773990818671942 0.4207721499038046\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.1449 - mse: 0.1449\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.1475 - mse: 0.1475\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.1485 - mse: 0.1485\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.1471 - mse: 0.1471\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.1486 - mse: 0.14860s - loss: 0.1502 - mse: 0.\n",
      "0.05 2 50\n",
      "0.7538757 0.1107014 0.046218872 0.1013151619535821 0.5032650665014349\n",
      "0.05 2 75\n",
      "0.7721651 0.12277866 0.04744691 0.08052931109254173 0.4541686277071691\n",
      "0.05 2 100\n",
      "0.8684813 0.14511076 0.0505455 0.062446846148691504 0.34749889590893357\n",
      "0.05 2 125\n",
      "0.9471826 0.13084947 0.09552797 0.06324134767859599 0.3232300075870377\n",
      "0.05 2 150\n",
      "0.85539913 0.11494702 0.071990505 0.05560426219467684 0.37969902583380233\n",
      "0.05 2 175\n",
      "0.8465978 0.12283717 0.0804287 0.06496647071949742 0.4197786863862752\n",
      "0.05 2 200\n",
      "0.8537929 0.11439999 0.090292044 0.05243552043554561 0.3426653844498328\n",
      "0.05 2 225\n",
      "0.958649 0.13660572 0.10078979 0.053949364308441966 0.35805883257812654\n",
      "0.05 2 250\n",
      "0.8685325 0.13063021 0.08544527 0.04843581620618334 0.38090577719849433\n",
      "0.05 2 275\n",
      "0.84832966 0.14739114 0.08561572 0.05762804750401549 0.4368484857176201\n",
      "0.05 2 300\n",
      "0.81357944 0.11417712 0.08412 0.04504084185990067 0.3936040857664027\n",
      "0.05 2 325\n",
      "0.8579242 0.14584225 0.10161567 0.04609279457238372 0.35489586805655426\n",
      "0.05 2 350\n",
      "0.85171974 0.13039571 0.1033062 0.050975595765517835 0.3895749817021015\n",
      "0.05 2 375\n",
      "0.8702315 0.14360741 0.09178677 0.0416394544799648 0.3658886709723923\n",
      "0.05 2 400\n",
      "0.92343646 0.14014637 0.11198929 0.048137150562527355 0.36187233299869687\n",
      "0.05 2 425\n",
      "0.8548516 0.12880987 0.092796445 0.0392741201066079 0.38030996889483826\n",
      "0.05 2 450\n",
      "0.8230536 0.11724121 0.094499364 0.03535055953412707 0.35926279606487116\n",
      "0.05 2 475\n",
      "0.86324596 0.11581034 0.094990864 0.042586256600886906 0.4469358933556327\n",
      "Building model with sigmoid activation and 0.125 dropout\n",
      "0.125 2 0\n",
      "WARNING:tensorflow:Layer flatten_78 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "8.438783e-07 4.3535664e-09 1.7993123e-09 1.1181669407924684 1.9043326908295333\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 2s 119us/sample - loss: 0.7916 - mse: 0.7916\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.6551 - mse: 0.6551\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.6436 - mse: 0.6436\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.6329 - mse: 0.6329\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6095 - mse: 0.6095\n",
      "0.125 2 5\n",
      "0.36502856 0.00424126 0.0008670214 0.5496711590586953 1.2254503300551587\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.5890 - mse: 0.5890\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.5497 - mse: 0.5497\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.5165 - mse: 0.5165\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 64us/sample - loss: 0.5032 - mse: 0.50320s - loss: 0.511\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.4782 - mse: 0.4782\n",
      "0.125 2 10\n",
      "0.5015863 0.028224153 0.0037653667 0.39130676633164924 0.9403774732476889\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.4329 - mse: 0.4329\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.3929 - mse: 0.3929\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.3722 - mse: 0.3722\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.3620 - mse: 0.3620\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.3332 - mse: 0.3332\n",
      "0.125 2 15\n",
      "0.83592165 0.11408768 0.040089715 0.25640887543648716 0.6521139170185823\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.3282 - mse: 0.3282\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.3179 - mse: 0.3179\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.3129 - mse: 0.3129\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.3156 - mse: 0.3156\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.3016 - mse: 0.3016\n",
      "0.125 2 20\n",
      "0.7825348 0.103005655 0.050578136 0.2203802269268097 0.6297168595653115\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.3010 - mse: 0.3010\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.2903 - mse: 0.2903\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.2958 - mse: 0.2958\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.2859 - mse: 0.2859\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.2819 - mse: 0.2819\n",
      "0.125 2 25\n",
      "0.87936527 0.10487073 0.053670034 0.18823998454543664 0.5093145230849172\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.2843 - mse: 0.2843\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.2766 - mse: 0.2766\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.2687 - mse: 0.2687\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.2716 - mse: 0.2716\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.2701 - mse: 0.2701\n",
      "0.125 2 30\n",
      "0.8616384 0.11621093 0.04521903 0.20830541625517293 0.575559026237518\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.2616 - mse: 0.2616\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.2624 - mse: 0.2624\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.2669 - mse: 0.2669\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 79us/sample - loss: 0.2568 - mse: 0.2568\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.2561 - mse: 0.2561\n",
      "0.125 2 35\n",
      "0.822584 0.10842468 0.02737049 0.17063396820089416 0.5254968409646315\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.2555 - mse: 0.2555\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 74us/sample - loss: 0.2575 - mse: 0.25750s - loss: 0.2599 - mse\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.2494 - mse: 0.2494\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.2513 - mse: 0.2513\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.2474 - mse: 0.2474\n",
      "0.125 2 40\n",
      "0.88745964 0.10111461 0.053498663 0.20146246899506173 0.5671806942611369\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 73us/sample - loss: 0.2393 - mse: 0.2393\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 65us/sample - loss: 0.2446 - mse: 0.2446\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.2473 - mse: 0.2473\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.2425 - mse: 0.24250s - loss: 0.2463 - mse: 0.\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.2373 - mse: 0.2373\n",
      "0.125 2 45\n",
      "0.73363453 0.104307614 0.043245073 0.1710685035263403 0.617227543336228\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.2386 - mse: 0.23860s - loss: 0.2415 - mse: 0.\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.2446 - mse: 0.2446\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 88us/sample - loss: 0.2359 - mse: 0.2359\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.2395 - mse: 0.2395\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.2321 - mse: 0.2321\n",
      "0.125 2 50\n",
      "0.73624355 0.09772219 0.050389938 0.1660982551055583 0.5992205657777179\n",
      "0.125 2 75\n",
      "0.80251133 0.10671848 0.059987947 0.16353230992129908 0.5710468557176451\n",
      "0.125 2 100\n",
      "0.8087996 0.12535845 0.05612896 0.20304905788256414 0.5719110609807702\n",
      "0.125 2 125\n",
      "0.8082883 0.112506576 0.06450772 0.1685187939295931 0.5195934032826551\n",
      "0.125 2 150\n",
      "0.65257543 0.10434336 0.05472174 0.17927641331160502 0.67437762327719\n",
      "0.125 2 175\n",
      "0.73547894 0.10448845 0.07057022 0.1653724165711577 0.5603153664364295\n",
      "0.125 2 200\n",
      "0.6941298 0.103582405 0.0747211 0.1633284221135248 0.5914529628806718\n",
      "0.125 2 225\n",
      "0.7728215 0.114848495 0.09375926 0.20888845671342307 0.648565679609042\n",
      "0.125 2 250\n",
      "0.6773688 0.105826534 0.0739264 0.15336513813253647 0.6109817669886564\n",
      "0.125 2 275\n",
      "0.7991438 0.13531387 0.08030003 0.2059944620871305 0.6028735833055708\n",
      "0.125 2 300\n",
      "0.6874985 0.114909776 0.07993811 0.1595499142410575 0.594139880697132\n",
      "0.125 2 325\n",
      "0.6899981 0.1329916 0.07432654 0.17187277359219771 0.6245861829530556\n",
      "0.125 2 350\n",
      "0.71732086 0.11508901 0.07309493 0.18889084434082398 0.6451649882730651\n",
      "0.125 2 375\n",
      "0.6649678 0.10828958 0.076220214 0.16004319942337417 0.6665812110379233\n",
      "0.125 2 400\n",
      "0.6761485 0.12050874 0.062471144 0.16550685700051262 0.64864259181235\n",
      "0.125 2 425\n",
      "0.6806838 0.13689978 0.06384141 0.16526816405100297 0.635785359056947\n",
      "0.125 2 450\n",
      "0.66154283 0.117019065 0.07018487 0.1655425988446976 0.6580844813330126\n",
      "0.125 2 475\n",
      "0.68427384 0.12158429 0.079427816 0.17536560920247382 0.6978797896710068\n",
      "Building model with sigmoid activation and 0.250 dropout\n",
      "0.25 2 0\n",
      "WARNING:tensorflow:Layer flatten_79 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "7.61738e-07 5.930949e-09 3.1610798e-09 1.263121119942259 2.97111785230867\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 72us/sample - loss: 0.8687 - mse: 0.8687\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 45us/sample - loss: 0.6993 - mse: 0.6993\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.6801 - mse: 0.6801\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 45us/sample - loss: 0.6737 - mse: 0.6737\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 45us/sample - loss: 0.6669 - mse: 0.6669\n",
      "0.25 2 5\n",
      "0.34612736 0.0038645158 0.00084786396 0.5791328367977641 1.266671312525338\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.6538 - mse: 0.6538\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.6268 - mse: 0.6268\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.6175 - mse: 0.6175\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.5970 - mse: 0.5970\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.5780 - mse: 0.5780\n",
      "0.25 2 10\n",
      "0.44377905 0.02225967 0.0021499838 0.48100396528560707 1.102476387786189\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.5708 - mse: 0.5708\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.5632 - mse: 0.5632\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.5581 - mse: 0.5581\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.5444 - mse: 0.5444\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.5310 - mse: 0.5310\n",
      "0.25 2 15\n",
      "0.70201916 0.047121093 0.0068138344 0.4163791917976181 0.9761044851142427\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.5139 - mse: 0.5139\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.5050 - mse: 0.5050\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.4699 - mse: 0.4699\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.4705 - mse: 0.4705\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.4571 - mse: 0.4571\n",
      "0.25 2 20\n",
      "0.6717042 0.06573005 0.019048339 0.4020359750956872 0.9800615791224526\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.4400 - mse: 0.4400\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.4505 - mse: 0.4505\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 45us/sample - loss: 0.4325 - mse: 0.4325\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.4389 - mse: 0.4390\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 46us/sample - loss: 0.4241 - mse: 0.4241\n",
      "0.25 2 25\n",
      "0.6549048 0.06962339 0.025118422 0.3990974814519137 1.0182407364091348\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.4176 - mse: 0.4176\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 67us/sample - loss: 0.4202 - mse: 0.4202\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 70us/sample - loss: 0.4290 - mse: 0.4290\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.4078 - mse: 0.4078\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.4122 - mse: 0.4122\n",
      "0.25 2 30\n",
      "0.7193714 0.07963218 0.027546149 0.413010512166701 1.0119614581252745\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13003/13003 [==============================] - 1s 72us/sample - loss: 0.4041 - mse: 0.4041\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.4073 - mse: 0.4073\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 70us/sample - loss: 0.3971 - mse: 0.3971\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.3960 - mse: 0.3960\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.3914 - mse: 0.3914\n",
      "0.25 2 35\n",
      "0.74932235 0.097393475 0.032951783 0.45773534395961185 1.1443252311357195\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.3971 - mse: 0.3971\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.3892 - mse: 0.3892\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.3791 - mse: 0.3791\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.3839 - mse: 0.3839\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.3875 - mse: 0.3875\n",
      "0.25 2 40\n",
      "0.67576987 0.08399304 0.026200464 0.41052337502663117 1.0859178368938043\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 62us/sample - loss: 0.3868 - mse: 0.3868\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.3857 - mse: 0.3857\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 58us/sample - loss: 0.3614 - mse: 0.3614\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.3679 - mse: 0.3679\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.3732 - mse: 0.3732\n",
      "0.25 2 45\n",
      "0.6017968 0.06599129 0.025817841 0.40920283867581936 1.1111853720604612\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.3735 - mse: 0.3735\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.3671 - mse: 0.3671\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.3695 - mse: 0.3695\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.3664 - mse: 0.3664\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.3709 - mse: 0.3709\n",
      "0.25 2 50\n",
      "0.665685 0.081236355 0.02894961 0.4631696603292129 1.1553043193164905\n",
      "0.25 2 75\n",
      "0.87293714 0.109262906 0.05468649 0.5245666966875104 1.0936614201728876\n",
      "0.25 2 100\n",
      "0.7724791 0.12130513 0.06250499 0.5475159101750067 1.1680369994725754\n",
      "0.25 2 125\n",
      "0.6013049 0.08394732 0.054972395 0.5134052887388032 1.2271119312629866\n",
      "0.25 2 150\n",
      "0.650908 0.10406867 0.06071583 0.5256342758883993 1.1765100687783534\n",
      "0.25 2 175\n",
      "0.57846713 0.10975054 0.067479484 0.6030619352294825 1.3038902315287737\n",
      "0.25 2 200\n",
      "0.7821983 0.11846609 0.07107376 0.5668644029696726 1.1156705976084946\n",
      "0.25 2 225\n",
      "0.64264506 0.11133037 0.06292392 0.5195056850234413 1.1447496768951444\n",
      "0.25 2 250\n",
      "0.6357666 0.11642484 0.0558319 0.5371307410061185 1.1174475399949109\n",
      "0.25 2 275\n",
      "0.60419774 0.1311582 0.06731872 0.6041785670264187 1.2889030510322554\n",
      "0.25 2 300\n",
      "0.5645272 0.11113698 0.065169275 0.5732275747968953 1.2185319000234092\n",
      "0.25 2 325\n",
      "0.69747394 0.1328524 0.08133911 0.5812617917034615 1.2229109529008724\n",
      "0.25 2 350\n",
      "0.53352714 0.1254236 0.05919077 0.5048253344430753 1.1265339663165037\n",
      "0.25 2 375\n",
      "0.654964 0.13894731 0.062279485 0.5497985614342983 1.183608706945555\n",
      "0.25 2 400\n",
      "0.6773058 0.123289 0.06660075 0.551648060283598 1.1368658442399664\n",
      "0.25 2 425\n",
      "0.71949905 0.16078071 0.075579286 0.5920071785969486 1.1848488842155842\n",
      "0.25 2 450\n",
      "0.6621663 0.14514194 0.06761732 0.5885446416093871 1.2628282553113133\n",
      "0.25 2 475\n",
      "0.6298159 0.12785336 0.066338606 0.5568896022670322 1.1874233901786986\n",
      "Building model with sigmoid activation and 0.375 dropout\n",
      "0.375 2 0\n",
      "WARNING:tensorflow:Layer flatten_80 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1.6751286e-06 7.502105e-09 4.134306e-09 1.0024979344961402 2.105130124476952\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 3s 203us/sample - loss: 0.9970 - mse: 0.9970\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 98us/sample - loss: 0.7551 - mse: 0.7551\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 70us/sample - loss: 0.7380 - mse: 0.7380\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 66us/sample - loss: 0.7156 - mse: 0.7156\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 67us/sample - loss: 0.7091 - mse: 0.7091\n",
      "0.375 2 5\n",
      "0.2644064 0.0025520404 0.00055631413 0.5879541079677311 1.3010582899737515\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.7011 - mse: 0.7011\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.6922 - mse: 0.6922\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.6835 - mse: 0.6835\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.6582 - mse: 0.6582\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.6570 - mse: 0.6570\n",
      "0.375 2 10\n",
      "0.28811204 0.010643056 0.0007806364 0.5444094594731165 1.2266841547672536\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6444 - mse: 0.6444\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.6397 - mse: 0.6397\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 57us/sample - loss: 0.6318 - mse: 0.6318\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.6270 - mse: 0.6270\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 59us/sample - loss: 0.6265 - mse: 0.6265\n",
      "0.375 2 15\n",
      "0.3813614 0.028131528 0.0026485936 0.5258474593622079 1.1729891297782866\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6115 - mse: 0.6115\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6204 - mse: 0.6204\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6135 - mse: 0.6135\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.6115 - mse: 0.6115\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 55us/sample - loss: 0.6060 - mse: 0.6060\n",
      "0.375 2 20\n",
      "0.532435 0.04831642 0.005899391 0.5058962306286402 1.1211595907599081\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6093 - mse: 0.6093\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.5964 - mse: 0.5964\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.5904 - mse: 0.5904\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.5831 - mse: 0.5831\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.5871 - mse: 0.5871\n",
      "0.375 2 25\n",
      "0.4044117 0.04290232 0.0062787766 0.5448128253095227 1.229353858495097\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.5718 - mse: 0.5718\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 53us/sample - loss: 0.5743 - mse: 0.5743\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 63us/sample - loss: 0.5706 - mse: 0.5706\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 62us/sample - loss: 0.5672 - mse: 0.5672\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.5582 - mse: 0.5582\n",
      "0.375 2 30\n",
      "0.38894892 0.04326461 0.0070050186 0.5211079623324422 1.222781432126169\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.5612 - mse: 0.5612\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.5439 - mse: 0.5439\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.5475 - mse: 0.5475\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.5377 - mse: 0.5377\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.5480 - mse: 0.5480\n",
      "0.375 2 35\n",
      "0.52064794 0.05075239 0.008441431 0.5677547923541996 1.2790616789905422\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.5406 - mse: 0.5406\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.5184 - mse: 0.5184\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.5273 - mse: 0.5273\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.5213 - mse: 0.5213\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.5332 - mse: 0.5332\n",
      "0.375 2 40\n",
      "0.3461773 0.033733476 0.006624836 0.6224451922112709 1.4214299580111778\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.5262 - mse: 0.5262\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.5293 - mse: 0.5293\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.5212 - mse: 0.5212\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.5136 - mse: 0.5136\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.5105 - mse: 0.5105\n",
      "0.375 2 45\n",
      "0.5464986 0.056004904 0.010430547 0.6393583808897367 1.3845269706579346\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.5054 - mse: 0.5054\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.5069 - mse: 0.5069\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.5143 - mse: 0.5143\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.5093 - mse: 0.5093\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 52us/sample - loss: 0.4966 - mse: 0.4966\n",
      "0.375 2 50\n",
      "0.5041351 0.05730781 0.012278024 0.7581489362586725 1.5844122621691898\n",
      "0.375 2 75\n",
      "0.53106564 0.083101526 0.017336708 0.79007671891324 1.6977580947812634\n",
      "0.375 2 100\n",
      "0.6024078 0.08445212 0.029185383 0.8325501354693232 1.7202210679592502\n",
      "0.375 2 125\n",
      "0.5539948 0.079174206 0.02434177 0.9198457654721837 1.8701367696163242\n",
      "0.375 2 150\n",
      "0.5542032 0.09352946 0.023752533 0.8959816807344707 1.8575434914114577\n",
      "0.375 2 175\n",
      "0.5788574 0.10942309 0.043740056 0.941347640315734 1.9271310470571619\n",
      "0.375 2 200\n",
      "0.5553623 0.10324872 0.032525778 0.9036916066681887 1.8701772333541375\n",
      "0.375 2 225\n",
      "0.6809197 0.133021 0.044999767 1.023304642765276 1.982640435010755\n",
      "0.375 2 250\n",
      "0.62063557 0.1464141 0.04726565 1.0259189058395446 2.038140056455907\n",
      "0.375 2 275\n",
      "0.6479867 0.11905013 0.04970528 1.0155173215233613 1.9945358095951513\n",
      "0.375 2 300\n",
      "0.6656185 0.14171086 0.046744075 1.0369485194688557 2.0142110887940055\n",
      "0.375 2 325\n",
      "0.5959968 0.10731256 0.045595292 0.9329507079995666 1.8544518057951491\n",
      "0.375 2 350\n",
      "0.64072853 0.123275146 0.046058837 1.0196469427952537 1.9301041752655435\n",
      "0.375 2 375\n",
      "0.593425 0.116480716 0.04914877 0.9997284086328807 1.9503936928940622\n",
      "0.375 2 400\n",
      "0.5972761 0.11869697 0.048888877 0.9715641866473481 1.9182517367136198\n",
      "0.375 2 425\n",
      "0.6430805 0.11528365 0.05172357 1.0117051737873626 1.9776233730706663\n",
      "0.375 2 450\n",
      "0.5773817 0.10413986 0.054738715 0.9701049220733946 1.8941992407611103\n",
      "0.375 2 475\n",
      "0.6826886 0.14949295 0.06255607 1.0517170252956678 2.0768661784914846\n",
      "Building model with sigmoid activation and 0.500 dropout\n",
      "0.5 2 0\n",
      "WARNING:tensorflow:Layer flatten_81 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1.4363433e-06 5.3506186e-09 2.2855877e-09 1.5131626469843824 3.4394393985800717\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 76us/sample - loss: 1.1165 - mse: 1.1165\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.8194 - mse: 0.8194\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.7935 - mse: 0.7935\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.7828 - mse: 0.7828\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.7710 - mse: 0.7710\n",
      "0.5 2 5\n",
      "0.17076142 0.001918822 0.0003730463 0.6284228133713059 1.3945143321670814\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.7608 - mse: 0.7608\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.7475 - mse: 0.7475\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.7480 - mse: 0.7480\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.7460 - mse: 0.7460\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.7375 - mse: 0.7375\n",
      "0.5 2 10\n",
      "0.2184722 0.004084255 0.00063034875 0.6125637636310628 1.3262142883134949\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.7224 - mse: 0.7224\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.7279 - mse: 0.7279\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.7260 - mse: 0.7260\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.7125 - mse: 0.7125\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.7056 - mse: 0.7056\n",
      "0.5 2 15\n",
      "0.16300644 0.008059145 0.00072830124 0.61343254575268 1.410994974817312\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.7102 - mse: 0.7102\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.6912 - mse: 0.6912\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.6945 - mse: 0.6945\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.6943 - mse: 0.6943\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.6913 - mse: 0.6913\n",
      "0.5 2 20\n",
      "0.18331753 0.017418263 0.0012184235 0.6097773687048228 1.3545035383627042\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.6971 - mse: 0.6971\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.6871 - mse: 0.6871\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6818 - mse: 0.6818\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.6848 - mse: 0.6848\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.6860 - mse: 0.6860\n",
      "0.5 2 25\n",
      "0.23258515 0.028041279 0.0019939255 0.5941641443314322 1.3181498076973834\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 60us/sample - loss: 0.6864 - mse: 0.6864\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.6862 - mse: 0.6862\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13003/13003 [==============================] - 1s 54us/sample - loss: 0.6799 - mse: 0.6799\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.6690 - mse: 0.6690\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.6682 - mse: 0.6682\n",
      "0.5 2 30\n",
      "0.21924996 0.032896604 0.0020571484 0.6068930908186204 1.338918746899063\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6642 - mse: 0.6642\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.6797 - mse: 0.6797\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.6706 - mse: 0.6706\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.6616 - mse: 0.6616\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.6686 - mse: 0.6686\n",
      "0.5 2 35\n",
      "0.24121062 0.034756828 0.0019475244 0.6177896142821553 1.328066307477826\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 50us/sample - loss: 0.6731 - mse: 0.6731\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.6646 - mse: 0.6646\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 56us/sample - loss: 0.6752 - mse: 0.6752\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 51us/sample - loss: 0.6624 - mse: 0.6624\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.6617 - mse: 0.6617\n",
      "0.5 2 40\n",
      "0.20700327 0.03899188 0.0022244824 0.626123644108845 1.371352189536304\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.6648 - mse: 0.6648\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.6611 - mse: 0.6611\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.6545 - mse: 0.6545\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.6581 - mse: 0.6581\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.6601 - mse: 0.6601\n",
      "0.5 2 45\n",
      "0.26577738 0.044327494 0.0023270815 0.6401139437705405 1.3389408583796283\n",
      "Train on 13003 samples\n",
      "Epoch 1/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.6638 - mse: 0.6638\n",
      "Epoch 2/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.6530 - mse: 0.6530\n",
      "Epoch 3/5\n",
      "13003/13003 [==============================] - 1s 47us/sample - loss: 0.6503 - mse: 0.6503\n",
      "Epoch 4/5\n",
      "13003/13003 [==============================] - 1s 48us/sample - loss: 0.6405 - mse: 0.6405\n",
      "Epoch 5/5\n",
      "13003/13003 [==============================] - 1s 49us/sample - loss: 0.6450 - mse: 0.6450\n",
      "0.5 2 50\n",
      "0.17672734 0.03462783 0.002031606 0.6909409850046563 1.425194085092304\n",
      "0.5 2 75\n",
      "0.26722294 0.051699176 0.0039176773 0.8867291115657006 1.737519645438025\n",
      "0.5 2 100\n",
      "0.27788365 0.048606467 0.005046385 0.9330576705600547 1.8573154393691318\n",
      "0.5 2 125\n",
      "0.28510937 0.059519824 0.0063909614 1.032175002177211 1.9978348959895655\n",
      "0.5 2 150\n",
      "0.22620736 0.044774175 0.0060389126 0.9067762941219035 1.9044198464263848\n",
      "0.5 2 175\n",
      "0.35350975 0.077807486 0.011196766 1.0734295038025714 2.1868511835551474\n",
      "0.5 2 200\n",
      "0.32193634 0.06675984 0.010910186 1.111019503924477 2.1754184035287993\n",
      "0.5 2 225\n",
      "0.3195499 0.06859885 0.011564987 1.1101499222411642 2.2252373998657635\n",
      "0.5 2 250\n",
      "0.36066487 0.064535946 0.014147078 1.1196338120518823 2.226869298957717\n",
      "0.5 2 275\n",
      "0.3132257 0.059746835 0.011989849 1.1348781315173198 2.280182674935283\n",
      "0.5 2 300\n",
      "0.36530948 0.071889706 0.014026291 1.1822457378517826 2.2700231649643925\n",
      "0.5 2 325\n",
      "0.3320134 0.056743532 0.011439249 1.1928052279432644 2.3907115396562633\n",
      "0.5 2 350\n",
      "0.38519785 0.06572053 0.013073984 1.1447999038806307 2.225793416819676\n",
      "0.5 2 375\n",
      "0.44493198 0.069974534 0.013977673 1.1766333026617048 2.297449410365948\n",
      "0.5 2 400\n",
      "0.4162687 0.07614313 0.014774622 1.240554145636305 2.3577878059181843\n",
      "0.5 2 425\n",
      "0.37724075 0.075916044 0.015926318 1.1686487445939597 2.3163070448927203\n",
      "0.5 2 450\n",
      "0.35603142 0.062068347 0.015255268 1.17179530923396 2.313414877524814\n",
      "0.5 2 475\n",
      "0.3588504 0.069449484 0.014725323 1.1739593969829276 2.3136583821249563\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 500\n",
    "with open(\"results/results_bikeshare_128.tsv\", 'a') as results_file:\n",
    "    for fit_iter in range(3):\n",
    "        for dropout_rate in [0.0, 0.05, 0.125, 0.25, 0.375, 0.5]:\n",
    "            model = build_model('sigmoid', dropout_rate, 128)\n",
    "            epoch = 0\n",
    "            while epoch < max_epoch:\n",
    "                print(dropout_rate, fit_iter, epoch)\n",
    "                pred_train = model(X_train, training=False).numpy()\n",
    "                pred_test  = model(X_test, training=False).numpy()\n",
    "                pred_query = model(X_query, training=False).numpy()\n",
    "                xgb1 = xgb(max_depth=1, n_estimators=1000)\n",
    "                xgb2 = xgb(max_depth=2, n_estimators=1000)\n",
    "                xgb3 = xgb(max_depth=3, n_estimators=1000)\n",
    "\n",
    "                xgb1.fit(X_test, pred_test)\n",
    "                xgb1_preds = xgb1.predict(X_test)\n",
    "\n",
    "                xgb2.fit(X_test, pred_test)\n",
    "                xgb2_preds = xgb2.predict(X_test)\n",
    "\n",
    "                xgb3.fit(X_test, pred_test)\n",
    "                xgb3_preds = xgb3.predict(X_test)\n",
    "\n",
    "                xgb1_var = np.var(xgb1_preds)\n",
    "                xgb2_var = np.var(xgb1_preds - xgb2_preds)\n",
    "                xgb3_var = np.var(xgb2_preds - xgb3_preds)\n",
    "\n",
    "                train_mse = np.mean(np.square(Y_train - np.squeeze(model(X_train, training=False).numpy())))\n",
    "                val_mse = np.mean(np.square(Y_test - np.squeeze(model(X_test, training=False).numpy())))\n",
    "\n",
    "                print(xgb1_var, xgb2_var, xgb3_var, train_mse, val_mse)\n",
    "                print('{:.3f}\\t{:d}\\t{:d}\\t{}\\t{}\\t{}\\t{}\\t{}'.format(\n",
    "                                dropout_rate, fit_iter+2, epoch,\n",
    "                    xgb1_var, xgb2_var, xgb3_var, train_mse, val_mse),\n",
    "                    file=results_file, flush=True)\n",
    "                if epoch < 50:\n",
    "                    model.fit(X_train, Y_train, epochs=5, verbose=1)\n",
    "                    epoch += 5\n",
    "                elif epoch < max_epoch:\n",
    "                    model.fit(X_train, Y_train, epochs=25, verbose=0)\n",
    "                    epoch += 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30551311924184116\n",
      "0.8065913772010289\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.square(Y_train - xgb1.predict(X_train))))\n",
    "print(np.mean(np.square(Y_test - xgb1.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14644779293972754\n",
      "0.5713745209092453\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.square(Y_train - xgb2.predict(X_train))))\n",
    "print(np.mean(np.square(Y_test - xgb2.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1342.,  706.,  756.,  547.,  341.,  266.,  169.,  118.,   96.,\n",
       "          35.]),\n",
       " array([-1.03690864, -0.42986534,  0.17717796,  0.78422126,  1.39126456,\n",
       "         1.99830785,  2.60535115,  3.21239445,  3.81943775,  4.42648105,\n",
       "         5.03352435]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.hist(model(X_train).numpy())\n",
    "plt.hist(xgb1.predict(X_train))\n",
    "plt.hist(xgb2.predict(X_train))\n",
    "plt.hist(Y_train, fill=False)\n",
    "plt.hist(Y_test, fill=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   7.,  134.,  725., 2423., 3098., 3272., 2196.,  903.,  223.,\n",
       "          22.]),\n",
       " array([-1.1371636 , -0.9117448 , -0.6863259 , -0.460907  , -0.23548813,\n",
       "        -0.01006925,  0.21534963,  0.4407685 ,  0.6661874 ,  0.8916063 ,\n",
       "         1.1170251 ], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(xgb2.predict(X_train) - xgb1.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.101641655"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(xgb2.predict(X_train) - xgb1.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030938728"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(xgb2.predict(X_query) - xgb1.predict(X_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 606., 3437., 4842., 6062.,  812.,  494.,  148.,  628.,  298.,\n",
       "          52.]),\n",
       " array([-0.9000373 , -0.78929144, -0.6785456 , -0.5677998 , -0.45705396,\n",
       "        -0.3463081 , -0.2355623 , -0.12481646, -0.01407063,  0.0966752 ,\n",
       "         0.20742103], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(xgb2.predict(X_query) - xgb1.predict(X_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
